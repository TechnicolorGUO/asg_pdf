{
    "title": "4 2 0 2",
    "authors": "Zongxi Li, Xianming Li, Qing Li, Espresso, Haoran Xie, Jing Li",
    "abstract": "Highquality sentence embeddings are fundamental in many natural language processing (NLP) tasks, such as semantic textual similarity (STS) and retrievalaugmented generation (RAG). Nevertheless, most existing methods leverage fixedlength embeddings from fulllayer language models, which lack the scalability to accommodate the diverse available resources across various applications. Viewing this gap, we propose a novel sentence embedding model Espresso Sentence Embeddings (ESE) with two learning processes. First, the learnto-express process encodes more salient representations to lower layers. Second, the learnto-compress process compacts essential features into the initial dimensions using Principal Component Analysis (PCA). This way, ESE can scale model depth via the former process and embedding size via the latter. Extensive experiments on STS and RAG suggest that ESE can effectively yield highquality sentence embeddings with less model depth and embedding size, enhancing inference efficiency. 2",
    "introduction": "Sentence embedding learning (Cer et al., 2018; Reimers and Gurevych, 2019; Gao et al., 2021; Li and Li, 2023, 2024) is a fundamental yet challenging task in the NLP research. It aims to capture essential semantic and syntactic information in natural language, benefiting various scenarios such as clustering (Reimers and Gurevych, 2019), semantic textual similarity (STS) (Gao et al., 2021; Li and Li, 2023), and retrievalaugmented generation (RAG) (Gao et al., 2023; Asai et al., 2023).\n\nIn the common deployment practices, the pipeline of applying sentence embeddings unfolds in two typical stages: (i) computing the sentence embeddings via a forward pass and (ii) employing these embeddings in downstream tasks. Existing work (Reimers and Gurevych, 2019; Gao et al., 2021; Li and Li, 2023, etc.) typically adopts entire Transformer (Vaswani et al., 2017) layers and full embedding sizes for all tasks, regardless of the varying resources and requirements across applications. It can result in computational redundancy and fails to scale well to the diverse resources available in downstream scenarios (Kusupati et al., 2022). To address this challenge, Matryoshka Representation Learning (MRL) concurrently trains multiple embeddings with cascading dimensions to enable scalable embedding sizes (Kusupati et al., 2022). However, MRL employs full Transformer layers for sentence embedding inference, which leads to high computational costs when using Large Language Models (LLMs) with very deep architectures (Li and Li, 2023; Wang et al., 2023; Li and Li, 2024).\n\nViewing these concerns, we propose a novel Espresso Sentence Embeddings (ESE) with two processes. First, the learnto-express process allocates more crucial representations to lower layers by weighting embeddings at different levels. Second, the learnto-compress process condenses essential\n\nPreprint. Under review.\n\nüëç............Inference Speed: SlowUtilization of embedding: SlowInference Speed: SlowUtilization of embedding: FastInference Speed: FastUtilization of embedding: FastTraditional Sentence EmbeddingMatryoshka Sentence EmbeddingEspresso Sentence Embedding......hidden layers\n\nhidden layersEmbedding Quality\n\nscalablehidden layersEmbedding Quality\n\nEmbedding QualityüëçTransformer Backbone\n\nFigure 1: The comparison of traditional (left), MRL (middle), and ESE (right) sentence embedding models. The gray blocks indicate Transformer layers finetuned in the full setting, while the coffeecolored ones indicate scalable settings. ESE can scale both model depth and embedding size.\n\nfeatures into initial dimensions by exploring the innerdependencies of embedding dimensions through Principal Component Analysis (PCA). Figure 1 depicts ESE‚Äôs overview and its differences from MRL and traditional sentence embedding methods. As depicted, ESE is designed to encode more salient information into lower layers and embedding dimensions. Thus, it enables scalable sentence embeddings that adapt to model depth and embedding size, allowing for more flexible accommodation of diverse computing resources. In contrast, MRL focuses solely on scaling embedding size, while traditional models lack scalability. In addition, the PCA implementation of ESE at various layers of embedding learning can help organize the learned features in order; it allows easier training than MRL, which simultaneously trains multiple varyingdimension embeddings to scale embedding sizes.\n\nTo the best of our knowledge, we are the first to learn sentence embedding with information compression, presenting scalable embedding inference to both model depths and embedding sizes.\n\nTo evaluate ESE, we extensively experiment across STS and RAG tasks. First, the main results on the STS benchmarks show that ESE performs competitively in full settings than nontrivial baselines and shows significantly better results with scaled model depth or embedding size. For instance, ESE enhances BERTbased BGE‚Äôs (Xiao et al., 2023) lowerlayer embeddings from 45.60 to 66.27. Then, ablation studies indicate that all modules positively affect ESE, and its performance is sensitive to smaller PCA compression sizes. Next, the RAG experiments show that ESE improves retrieval across varying embedding sizes and model depths. Finally, we further discuss ESE and present the following findings: (i) The scaled embeddings from ESE work better than trained embeddings with the samesized model; (ii) ESE shows better inference efficiency on both STS and RAG tasks; (iii) In visualization, ESE‚Äôs scaled embeddings exhibit greater overlap with the unscaled ones, indicating its effectiveness in information compression and interpreting its superiority in scaling embeddings.\n\nIn summary, our contributions are listed as follows:\n\nWe are the first to employ the information compression technique to scale sentence embeddings.\n\nOur novel ESE model allows for scalable embeddings in both model depth and embedding size.\n\nExtensive experimental results on STS and RAG show ESE‚Äôs superiority in producing effective embeddings with reduced model depth and embedding size, enhancing inference efficiency."
}