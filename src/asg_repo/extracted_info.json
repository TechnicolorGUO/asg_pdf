{
    "title": "4 2 0 2",
    "authors": "Xianming Li, Jing Li",
    "abstract": "NONE",
    "introduction": "The development of text embeddings (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Cer et al., 2018; Reimers & Gurevych, 2019; Gao et al., 2021) is an essential research challenge in the NLP community. Text embeddings effectively feature key semantic and syntactic information in language, which broadly affects the performance of downstream tasks, such as text classification (Li et al., 2021), sentiment analysis (Suresh & Ong, 2021; Zhang et al., 2022), semantic matching (Grill et al., 2020; Lu et al., 2020), clustering (Reimers & Gurevych, 2019; Xu et al., 2023), and questionanswering (QA) system (Yue et al., 2021). In particular, text embedding models play a crucial role in LLMs such as ChatGPT (OpenAI, 2022; 2023), LLaMA (Touvron et al., 2023a;b), and ChatGLM (Du et al., 2022)-based applications. These LLMbased applications heavily rely on highquality text embeddings for tasks such as vector search, where related documents are retrieved for LLM QA (Asai et al., 2023).\n\nRecent studies (Gao et al., 2021; Jiang et al., 2022b; Chuang et al., 2022; Chanchani & Huang, 2023; Zhuo et al., 2023) have utilized pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in combination with contrastive learning to enhance the quality of text embeddings. These approaches involve pulling semantically similar samples together and pushing apart those not (Gao et al., 2021). In these contrastive models, positive samples that are semantically similar can be generated by data augmentation, while negative samples that are dissimilar are selected from different texts within the same minibatch (inbatch negatives). However, supervised negatives are underutilized, and the correctness of inbatch negatives is difficult to guarantee without annotation, which can lead to performance degradation. Although some models such as (Gao et al., 2021) optimize hard negative samples, they rely on strict triple formats (xi, x+ i ). While most existing supervised STS datasets only provide pairs (xi, x+ refers to the i positive sample of xi while x− j the negative sample of xj. Thus, most contrastive models are used in unsupervised settings yet might not benefit from human supervision.\n\ni , x− j ), where x+\n\ni ) or (xj, x−\n\n∗Corresponding author"
}