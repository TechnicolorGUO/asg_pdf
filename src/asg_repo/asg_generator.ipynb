{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc50ee6a",
   "metadata": {},
   "source": [
    "Load-Split-Embed-Store-Retrieve (对应RAG工作流的0-3)\n",
    "1. Load：asg_loader中，输入pdf，输出解析出的documents (类似txt) 内容包含title authorlist abstract introduction四部分\n",
    "2. Split：asg_splitter中 (其中集成asg_loader)，输入pdf，输出多个分块后的documents\n",
    "3. Embed + Store：asg_retriever中 (其中集成asg_splitter)，输入pdf，效果是对于输入的pdf建立相应向量数据库。输出retrieve中需要的参数 这是第一个封装的函数\n",
    "4. Retrieve：asg_retriever中，输入用户的一个query，输出五个相似度最高检索结果 这是第二个封装的函数。\n",
    "\n",
    "评估标准？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aWW2-ZuB1APf",
   "metadata": {
    "id": "aWW2-ZuB1APf"
   },
   "source": [
    "## RAG工作流（7.14）\n",
    "\n",
    "0. 用户上传多个pdf，经过indexing以多个collection的形式存入chroma \n",
    "\n",
    "1. 用户提出问题，如“paper中都使用了什么方法？”。\n",
    "\n",
    "2. （可能不必要） 利用prompt engineering，通过LlaMa-3-8B将用户提出的问题rephrase，使之标准，易于大模型理解。\n",
    "\n",
    "4. 根据用户的问题，从数据库中检索与问题相关的文档片段，作为context。\n",
    "\n",
    "6. 使用检索到的context和用户的问题作为参数，输入到LlaMa-3-8B中生成最终回答，其中编写prompt engineering，提供详细且准确的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VTek6nzys_10",
   "metadata": {
    "id": "VTek6nzys_10"
   },
   "source": [
    "## HyDE工作流\n",
    "\n",
    "0. 用户上传多个pdf，经过indexing以多个collection的形式存入chroma。\n",
    "\n",
    "1. 用户提出问题，如“paper中都使用了什么方法？”。\n",
    "\n",
    "2. （可能不必要）利用prompt engineering，通过LlaMa-3-8B将用户提出的问题rephrase，使之标准，易于大模型理解。\n",
    "\n",
    "3. 使用LlaMa-3-8B（prompt engineering）生成一个假设文档，这个文档对提出的问题进行补充描述，从不同角度提供信息。例如，生成文档可能会提供一个上下文回答，关于写paper时有哪些经常使用的方法。\n",
    "\n",
    "4. 使用HuggingFace的embedding模型（sentence-transformer）将生成的假设文档进行文本嵌入。将生成的嵌入存储到Chroma数据库中。\n",
    "\n",
    "5. 根据用户的问题，从数据库中检索与问题相关的文档片段，作为context。\n",
    "\n",
    "\n",
    "6. 使用检索到的context（context是否要结合虚拟文档作为上下文？）和用户的问题作为参数，输入到LlaMa-3-8B中生成最终回答，其中编写prompt engineering，提供详细且准确的信息。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sfUahZpsd7lg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1721167828159,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "sfUahZpsd7lg",
    "outputId": "25d5def7-88d9-4962-f56f-b01a4cb81154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 17 11:55:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090         On | 00000000:0A:00.0 Off |                  N/A |\n",
      "|  0%   25C    P8               26W / 350W|      6MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090         On | 00000000:0B:00.0 Off |                  N/A |\n",
      "|  0%   56C    P8               34W / 350W|    370MiB / 24576MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1249      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      1249      G   /usr/lib/xorg/Xorg                           63MiB |\n",
      "|    1   N/A  N/A      2609      G   /usr/bin/gnome-shell                         70MiB |\n",
      "|    1   N/A  N/A      3267      G   /usr/bin/anydesk                             12MiB |\n",
      "|    1   N/A  N/A     11433      G   /usr/lib/firefox/firefox                    220MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# 调用 nvidia-smi 命令\n",
    "result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "print(result.stdout.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb74c9b",
   "metadata": {
    "id": "ccb74c9b"
   },
   "source": [
    "# HyDE\n",
    "\n",
    "Hypothetical Document Embeddings (HyDE) is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example, as described in [this paper](https://arxiv.org/abs/2212.10496).\n",
    "\n",
    "In order to use HyDE, we need to provide a base embedding model, as well as an LLMChain that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (the paper has more for details on them), but we can also create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572340ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cbbwF0mubx5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83339,
     "status": "ok",
     "timestamp": 1721169108733,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "9cbbwF0mubx5",
    "outputId": "97fbadfa-b477-4e1f-b9c0-af53fd653081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (2022.3.2)\n",
      "Requirement already satisfied: requests in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: torch in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: langchain in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (0.2.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (0.2.11)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langchain) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2021.5.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (2.4)\n",
      "Requirement already satisfied: langchain_community in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (0.2.6)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (0.6.6)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (0.2.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (0.2.11)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (0.1.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (1.24.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (2.8.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langchain_community) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langchain_community) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langchain_community) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langchain_community) (2021.5.30)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: langchain_core in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (0.2.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_core) (6.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_core) (0.1.83)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_core) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_core) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langchain_core) (8.3.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (3.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from pydantic<3,>=1->langchain_core) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2021.5.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install langchain\n",
    "!pip install langchain_community\n",
    "!pip install langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "JjgEiStquIcU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18600,
     "status": "ok",
     "timestamp": 1721167994008,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "JjgEiStquIcU",
    "outputId": "f6b5d374-d524-4ce4-9315-f40bf4b066b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e3f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "oW4lY3PbYT1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 8908,
     "status": "ok",
     "timestamp": 1721168007824,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "oW4lY3PbYT1m"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "PoB7CAQFgh4T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "d97186eb647b4b4ea6632d52aad60dd7",
      "61ca35879b714b7cbd228a74ea628272",
      "6a44a02e8c094960b079f19d63b4128d",
      "e8a64cd701ca49d494f3961750be71c0",
      "0be5e164895c4308bcbdd6c3d301c4d7",
      "8dc1a571cb4a4bdc9a06272be0ff8680",
      "e541de6bc68d4f17a1d758305a60d0ed",
      "10c59dd368ce478f95705af1df1776af",
      "a7bfd34bdf1946e79503265d27cfb264",
      "7bbd3a944f83430cb8f23c9715b86d4d",
      "17e9c58c45434cb0a6cc70d188ec5cd6"
     ]
    },
    "executionInfo": {
     "elapsed": 174852,
     "status": "ok",
     "timestamp": 1720978592514,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "PoB7CAQFgh4T",
    "outputId": "19f78be4-25d1-4bd3-d3df-d8d28b6df885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path of Llama-3 exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97186eb647b4b4ea6632d52aad60dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# import torch\n",
    "# import os\n",
    "# model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n",
    "# if os.path.exists(model_path):\n",
    "#     print(\"The path of Llama-3 exists.\")\n",
    "# else:\n",
    "#     print(\"The path of Llama-3 doesn't exist.\")\n",
    "\n",
    "# # load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# # let the the device be gpu (outside colab)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        if os.path.exists(self.model_path):\n",
    "            print(\"The path of Llama-3 exists.\")\n",
    "        else:\n",
    "            print(\"The path of Llama-3 doesn't exist.\")\n",
    "\n",
    "        # load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, local_files_only=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True)\n",
    "\n",
    "        # let the device be gpu (outside colab)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def rephrase(self, question, rephrase_num, temp=0.7):\n",
    "        \"\"\" Original version of rephrase function. \"\"\"\n",
    "\n",
    "        template = \"\"\"\n",
    "        You are an assistant tasked with taking a natural language query from a user and converting it into a query for a vectorstore. In this process, you strip out information that is not relevant for the retrieval task. Here is the user query: {question}\n",
    "        Rephrased Question: \"\"\"\n",
    "\n",
    "        prompt_template = PromptTemplate(template=template)\n",
    "        result = [question]\n",
    "        for i in range(rephrase_num):\n",
    "            inputs = prompt_template.format(question=question)\n",
    "            input_ids = self.tokenizer(inputs, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "            outputs = self.model.generate(input_ids, max_length=100, temperature=temp, pad_token_id=self.tokenizer.eos_token_id)\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # extract rephrased Question\n",
    "            start_token = \"Rephrased Question:\"\n",
    "            start_index = response.find(start_token) + len(start_token)\n",
    "            if start_index != -1:\n",
    "                rephrased_question = response[start_index:].strip().split('\\n')[0]\n",
    "                if len(rephrased_question) > 1:\n",
    "                    result.append(rephrased_question)\n",
    "\n",
    "        return result\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n",
    "generator = Generator(model_path)\n",
    "print(\"Successfully load the model.\")\n",
    "\n",
    "question_1 = \"What the hell are the methods used in the paper?\"\n",
    "rephrased_question_1 = generator.rephrase(question_1, rephrase_num=1)\n",
    "print(rephrased_question_1)\n",
    "\n",
    "question_2 = \"Can you tell me is the evaluation method used in article is 定性 or 定量?\"\n",
    "rephrased_question_2 = generator.rephrase(question_2, rephrase_num=1)\n",
    "print(rephrased_question_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ICHiD8ozk306",
   "metadata": {
    "id": "ICHiD8ozk306"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"The path of Llama-3 exists.\")\n",
    "    else:\n",
    "        print(\"The path of Llama-3 doesn't exist.\")\n",
    "\n",
    "    # load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "    # let the device be gpu (outside colab)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    return tokenizer, model, device\n",
    "\n",
    "tokenizer, model, device = load_model()\n",
    "\n",
    "def rephrase(question, rephrase_num, temp=0.7):\n",
    "    \"\"\" Original version of rephrase function. \"\"\"\n",
    "\n",
    "    template = \"\"\"\n",
    "    You are an assistant tasked with taking a natural language query from a user and converting it into a query for a vectorstore. In this process, you strip out information that is not relevant for the retrieval task. Here is the user query: {question}\n",
    "    Rephrased Question: \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(template=template)\n",
    "    result = [question]\n",
    "    for i in range(rephrase_num):\n",
    "        inputs = prompt_template.format(question=question)\n",
    "        input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        outputs = model.generate(input_ids, max_length=100, temperature=temp, pad_token_id=tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # extract rephrased Question\n",
    "        start_token = \"Rephrased Question:\"\n",
    "        start_index = response.find(start_token) + len(start_token)\n",
    "        if start_index != -1:\n",
    "            rephrased_question = response[start_index:].strip().split('\\n')[0]\n",
    "            if len(rephrased_question) > 1:\n",
    "                result.append(rephrased_question)\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"Successfully load the model.\")\n",
    "\n",
    "question_1 = \"What the hell are the methods used in the paper?\"\n",
    "rephrased_question_1 = rephrase(question_1, rephrase_num=1)\n",
    "print(rephrased_question_1)\n",
    "\n",
    "question_2 = \"Can you tell me is the evaluation method used in article is 定性 or 定量?\"\n",
    "rephrased_question_2 = rephrase(question_2, rephrase_num=2)\n",
    "print(rephrased_question_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tWQ88BzDnlFD",
   "metadata": {
    "id": "tWQ88BzDnlFD"
   },
   "outputs": [],
   "source": [
    "\n",
    "def rephrase(question, rephrase_num, temp=0.7):\n",
    "    \"\"\" Original version of rephrase function. \"\"\"\n",
    "\n",
    "    # HyDE requires hypothesis document, what's the format of the prompt?\n",
    "    # below is the default prompt used in the from_llm classmethod\n",
    "    template = \"\"\"\n",
    "    You are an assistant tasked with taking a natural language query from a user and converting it into a query for a vectorstore.\n",
    "    In this process, you strip out information that is not relevant for the retrieval task. Here is the user query: {question}\n",
    "    Rephrased Question: \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate(template=template)\n",
    "\n",
    "    result = [question]\n",
    "    for i in range(rephrase_num):\n",
    "        inputs = prompt_template.format(question=question)\n",
    "        input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        outputs = model.generate(input_ids, max_length=100, temperature=temp, pad_token_id=tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # extract rephrased Question\n",
    "        start_token = \"Rephrased Question:\"\n",
    "        start_index = response.find(start_token) + len(start_token)\n",
    "\n",
    "        if start_index != -1:\n",
    "            rephrased_question = response[start_index:].strip().split('\\n')[0]\n",
    "            if len(rephrased_question) > 1:\n",
    "                result.append(rephrased_question)\n",
    "\n",
    "    return result\n",
    "\n",
    "question_1 = \"What the hell are the methods used isssn the paper?\"\n",
    "rephrased_question_1 = rephrase(question_1, rephrase_num=1)\n",
    "print(rephrased_question_1)\n",
    "question_2 = \"Can you tell me is the the evaluation method used in article is 定性 or 定量?\"\n",
    "rephrased_question_2 = rephrase(question_2, rephrase_num=1)\n",
    "print(rephrased_question_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6Q8foPREwPME",
   "metadata": {
    "id": "6Q8foPREwPME"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n",
    "\n",
    "# need so many to download\n",
    "# from asg_splitter import TextSplitting\n",
    "# from asg_retriever import Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "022b04cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path '/home/guest01/develope/web/Meta-Llama-3-8B-Instruct/' exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_model_path_exists(model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"The path '{model_path}' exists.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"The path '{model_path}' does not exist.\")\n",
    "        return False\n",
    "\n",
    "# 示例路径\n",
    "model_path = \"/home/guest01/develope/web/Meta-Llama-3-8B-Instruct/\"\n",
    "\n",
    "# 检查路径是否存在\n",
    "path_exists = check_model_path_exists(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Jzq2YGD7vBVr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "error",
     "timestamp": 1721168442159,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "Jzq2YGD7vBVr",
    "outputId": "4f69bef9-f0bd-480b-bd25-f99e55cdc9ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d0cf4f8c3b4ede866ec3d8d19426e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest01/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/cuda/__init__.py:143: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 23.69 GiB total capacity; 23.24 GiB already allocated; 197.12 MiB free; 23.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12555/1880634978.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Who is Verun?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12555/1880634978.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(context, question, temp)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     pipe = pipeline(\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m                 )\n\u001b[0;32m-> 2796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asg_pdf/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 23.69 GiB total capacity; 23.24 GiB already allocated; 197.12 MiB free; 23.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def generate(context, question, temp=0.7):\n",
    "\n",
    "    model_path = \"/home/guest01/develope/web/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "    \n",
    "#     /home/guest01/develope/\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        batch_size=1,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=4,\n",
    "        do_sample=True,\n",
    "        top_p=0.8,\n",
    "        temperature=temp,\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    Provide a precise answer based on the context and attach the source coordinate SC of your answer in [SC]:\n",
    "    ```\n",
    "    ============================================================\n",
    "    @3603a49a-d26c-4a2d-a9f1-0a608211b3ba//Verun is a current year-3 student of CS//\n",
    "    @7628b67b-3eb1-437c-b302-f0df15847605//Verun is a boy//\n",
    "    @fbb63230-714f-4b53-bfcc-20ca1f4ff734//Verun is from San Diego//\n",
    "    END OF RESULT//\n",
    "    ============================================================\n",
    "\n",
    "    Question: Who is Verun?\n",
    "    ============================================================\n",
    "    ```\n",
    "    Think and respond with [@SC] in several complete and logical sentences:\n",
    "    ```\n",
    "    THOUGHT: The question ~~Who is Verun?~~ is asking for Verun's information.\n",
    "\n",
    "    ANSWER: Verun is a student from San Diego[SC: @fbb63230-714f-4b53-bfcc-20ca1f4ff734] and currently in CS[SC: @3603a49a-d26c-4a2d-a9f1-0a608211b3ba].\n",
    "\n",
    "    THOUGHT: 'Verun is a student from San Diego[SC: @fbb63230-714f-4b53-bfcc-20ca1f4ff734]' is not related to '@7628b67b-3eb1-437c-b302-f0df15847605//HAER is a boy//'.\n",
    "\n",
    "    FINAL ANSWER: As far as I know, Verun is a student from San Diego[SC: @e956dd] and currently in CS[SC: @0a3f01].\n",
    "    ```\n",
    "\n",
    "    Now do the real task below!\n",
    "\n",
    "    ============================================================\n",
    "    {context}\n",
    "    ============================================================\n",
    "    Question: {question}\n",
    "    ============================================================\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(template)\n",
    "    formatted_prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "    # generate answer using pipe\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model.generate(inputs, max_length=1000, temperature=temp)\n",
    "    res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract final answer directly in the generate function\n",
    "    final_answer_start = \"FINAL ANSWER:\"\n",
    "    start_index = res.find(final_answer_start)\n",
    "    if start_index != -1:\n",
    "        final_answer = res[start_index + len(final_answer_start):].strip()\n",
    "    else:\n",
    "        final_answer = \"No final answer found.\"\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "context = \"\"\"\n",
    "@3603a49a-d26c-4a2d-a9f1-0a608211b3ba//Verun is a current year-3 student of CS//\n",
    "@7628b67b-3eb1-437c-b302-f0df15847605//Verun is a boy//\n",
    "@fbb63230-714f-4b53-bfcc-20ca1f4ff734//Verun is from San Diego//\n",
    "END OF RESULT//\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who is Verun?\"\n",
    "\n",
    "result = generate(context, question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d6bb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e86ba67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: 24062 MiB / 24576 MiB used\n",
      "GPU 1: 325 MiB / 24576 MiB used\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_usage():\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'], stdout=subprocess.PIPE)\n",
    "    memory_usage = result.stdout.decode('utf-8').strip().split('\\n')\n",
    "    gpu_memory_info = [tuple(map(int, info.split(', '))) for info in memory_usage]\n",
    "    return gpu_memory_info\n",
    "\n",
    "gpu_memory_info = get_gpu_memory_usage()\n",
    "for idx, (used, total) in enumerate(gpu_memory_info):\n",
    "    print(f\"GPU {idx}: {used} MiB / {total} MiB used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HhrqZcB_nFOT",
   "metadata": {
    "id": "HhrqZcB_nFOT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 全局加载模型和分词器\n",
    "model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate(context, question, temp=0.7):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        batch_size=1,\n",
    "        max_new_tokens=200,\n",
    "        num_beams=4,\n",
    "        do_sample=True,\n",
    "        top_p=0.8,\n",
    "        temperature=temp,\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    Provide a precise answer based on the context and attach the source coordinate SC of your answer in [SC]:\n",
    "    ```\n",
    "    ============================================================\n",
    "    @3603a49a-d26c-4a2d-a9f1-0a608211b3ba//Verun is a current year-3 student of CS//\n",
    "    @7628b67b-3eb1-437c-b302-f0df15847605//Verun is a boy//\n",
    "    @fbb63230-714f-4b53-bfcc-20ca1f4ff734//Verun is from San Diego//\n",
    "    END OF RESULT//\n",
    "    ============================================================\n",
    "\n",
    "    Question: Who is Verun?\n",
    "    ============================================================\n",
    "    ```\n",
    "    Think and respond with [@SC] in several complete and logical sentences:\n",
    "    ```\n",
    "    THOUGHT: The question ~~Who is Verun?~~ is asking for Verun's information.\n",
    "\n",
    "    ANSWER: Verun is a student from San Diego[SC: @fbb63230-714f-4b53-bfcc-20ca1f4ff734] and currently in CS[SC: @3603a49a-d26c-4a2d-a9f1-0a608211b3ba].\n",
    "\n",
    "    THOUGHT: 'Verun is a student from San Diego[SC: @fbb63230-714f-4b53-bfcc-20ca1f4ff734]' is not related to '@7628b67b-3eb1-437c-b302-f0df15847605//HAER is a boy//'.\n",
    "\n",
    "    FINAL ANSWER: As far as I know, Verun is a student from San Diego[SC: @e956dd] and currently in CS[SC: @0a3f01].\n",
    "    ```\n",
    "\n",
    "    Now do the real task below!\n",
    "\n",
    "    ============================================================\n",
    "    {context}\n",
    "    ============================================================\n",
    "    Question: {question}\n",
    "    ============================================================\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(template)\n",
    "    formatted_prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "    # generate answer using pipe\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model.generate(inputs, max_length=300, temperature=temp)\n",
    "    res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract final answer directly in the generate function\n",
    "    final_answer_start = \"FINAL ANSWER:\"\n",
    "    start_index = res.find(final_answer_start)\n",
    "    if start_index != -1:\n",
    "        final_answer = res[start_index + len(final_answer_start):].strip()\n",
    "    else:\n",
    "        final_answer = \"No final answer found.\"\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "context = \"\"\"\n",
    "@3603a49a-d26c-4a2d-a9f1-0a608211b3ba//Verun is a current year-3 student of CS//\n",
    "@7628b67b-3eb1-437c-b302-f0df15847605//Verun is a boy//\n",
    "@fbb63230-714f-4b53-bfcc-20ca1f4ff734//Verun is from San Diego//\n",
    "END OF RESULT//\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who is Verun?\"\n",
    "\n",
    "result = generate(context, question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Tjo5KQrjTK7D",
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1720979584543,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "Tjo5KQrjTK7D"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_YwX9v4JTs37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6213,
     "status": "ok",
     "timestamp": 1720661936288,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "_YwX9v4JTs37",
    "outputId": "4033632b-7d7a-40f4-b57f-8441bb9aa7b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. \n",
      "Provide only a precise answer based on the context and attach the source coordinate SC of your answer in [SC]:\n",
      "# Answer with source [SC]:\n",
      "Shuki asked Verun if he is from San Diego. He said yes.     Ayden is a current year 3 student in the department of COMP at POLYU.     Verun is a roommate of Ayden.     He is a current year 3 student in the department of CS at Stanford University.\n",
      "Question: Who is Verun?\n",
      "Think and respond with [@SC] in several complete and logical sentences. \n",
      "Answer:\n",
      "Verun is a current year 3 student in the department of CS at Stanford University. He is also a roommate of Ayden, who is a current year 3 student in the department of COMP at POLYU. Additionally, Verun confirmed that he is from San Diego when Shuki asked him. [@SC] \n",
      "Source Coordinate: [SC: 1] \n",
      "Note: SC refers to the source coordinate of the answer, which is the number of the sentence in the original text where the information is found. In this case, the answer is based on sentences 1, 3, and 4.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "Provide only a precise answer based on the context and attach the source coordinate SC of your answer in [SC]:\n",
    "# Answer with source [SC]:\n",
    "{context}\n",
    "Question: {question}\n",
    "Think and respond with [@SC] in several complete and logical sentences.\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# an simple example\n",
    "context = \"Shuki asked Verun if he is from San Diego. He said yes. \\\n",
    "    Ayden is a current year 3 student in the department of COMP at POLYU. \\\n",
    "    Verun is a roommate of Ayden. \\\n",
    "    He is a current year 3 student in the department of CS at Stanford University.\"\n",
    "question = \"Who is Verun?\"\n",
    "\n",
    "formatted_prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "# generate the answer\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(inputs, max_length=300, temperature=0.7)\n",
    "res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gte5nFyFTsyX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11301,
     "status": "ok",
     "timestamp": 1720671778246,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "Gte5nFyFTsyX",
    "outputId": "aa933f9d-beaf-447a-bad7-9f7f96e8bb5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please write an introductory passage to answer the question:\n",
      "Question: Where is Hong Kong?\n",
      "Passage:\n",
      "Hong Kong is a Special Administrative Region of China, located on the southeastern coast of the country. It is situated on the Pearl River Delta, which is one of the most densely populated regions in the world. Hong Kong is an archipelago, consisting of over 260 islands, with the largest being Hong Kong Island, Lantau Island, and the Kowloon Peninsula. The region is bordered by the Guangdong Province of China to the north, and the South China Sea to the south, east, and west. Hong Kong is a major financial and trade center, and its unique blend of East and West cultures has made it a popular tourist destination.\n",
      "\n",
      "Please note that the passage should be around 150-200 words. \n",
      "\n",
      "Here is the passage:\n",
      "\n",
      "Hong Kong is a Special Administrative Region of China, located on the southeastern coast of the country. It is situated on the Pearl River Delta, which is one of the most densely populated regions in the world. Hong Kong is an archipelago, consisting of over 260 islands, with the largest being Hong Kong Island, Lantau Island, and the Kowloon Peninsula. The region is bordered by the Guangdong Province of China to the north, and the South China Sea to the south, east, and west. Hong Kong is a major financial and trade center, and its unique blend of East and West cultures has made\n"
     ]
    }
   ],
   "source": [
    "# HyDE prompt example\n",
    "template = \"\"\"\n",
    "Please write an introductory passage to answer the question:\n",
    "Question: {question}\n",
    "Passage:\n",
    "\"\"\"\n",
    "\n",
    "# define the virtual document generation Prompt template\n",
    "prompt_hyde_template = template\n",
    "\n",
    "# example question\n",
    "question = \"Where is Hong Kong?\"\n",
    "\n",
    "# generate virtual document\n",
    "formatted_prompt = prompt_hyde_template.format(question=question)\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(inputs, max_length=300, temperature=0.7)\n",
    "virtual_document = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(virtual_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ikX-glCU3F2P",
   "metadata": {
    "id": "ikX-glCU3F2P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oqDsVfUs3Fok",
   "metadata": {
    "id": "oqDsVfUs3Fok"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1XLf7P-w3FbC",
   "metadata": {
    "id": "1XLf7P-w3FbC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rOCj2FDT3FXq",
   "metadata": {
    "id": "rOCj2FDT3FXq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bW6BRLlW3FUY",
   "metadata": {
    "id": "bW6BRLlW3FUY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D21jFUDE3FRS",
   "metadata": {
    "id": "D21jFUDE3FRS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e87ee",
   "metadata": {
    "id": "546e87ee"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import HypotheticalDocumentEmbedder, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A8DvNuZ7lvXf",
   "metadata": {
    "id": "A8DvNuZ7lvXf"
   },
   "outputs": [],
   "source": [
    "class HypotheticalDocumentEmbedder:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        inputs = self.tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        embeddings = outputs.hidden_states[-1][:, 0, :].squeeze().detach().cpu().numpy()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HXDoCTG2lyHj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1720597340440,
     "user": {
      "displayName": "Zian WANG",
      "userId": "02436955510043631970"
     },
     "user_tz": 420
    },
    "id": "HXDoCTG2lyHj",
    "outputId": "af11fc5d-1e22-4dc5-b29b-ce017da64135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'Where is the Taj Mahal?': [ 4.185107   -0.20592627 -1.8382323  ... -2.890834    1.3604962\n",
      "  0.31094578]\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "# 实例化嵌入类\n",
    "embedder = HypotheticalDocumentEmbedder(model, tokenizer)\n",
    "\n",
    "# 嵌入查询示例\n",
    "query = \"Where is the Taj Mahal?\"\n",
    "embedding = embedder.embed_query(query)\n",
    "print(f\"Embedding for '{query}': {embedding}\")\n",
    "print(len(embedding)) # 4096维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea895f",
   "metadata": {
    "id": "c0ea895f"
   },
   "outputs": [],
   "source": [
    "base_embeddings = OpenAIEmbeddings()\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50729989",
   "metadata": {
    "id": "50729989"
   },
   "outputs": [],
   "source": [
    "# Load with `web_search` prompt\n",
    "embeddings = HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, \"web_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa573d6",
   "metadata": {
    "id": "3aa573d6"
   },
   "outputs": [],
   "source": [
    "# Now we can use it as any embedding class!\n",
    "result = embeddings.embed_query(\"Where is the Taj Mahal?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da90437",
   "metadata": {
    "id": "1da90437"
   },
   "source": [
    "## Using our own prompts\n",
    "Besides using preconfigured prompts, we can also easily construct our own prompts and use those in the LLMChain that is generating the documents. This can be useful if we know the domain our queries will be in, as we can condition the prompt to generate text more similar to that.\n",
    "\n",
    "In the example below, let's condition it to generate text about a state of the union address (because we will use that in the next example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a650f",
   "metadata": {
    "id": "0b4a650f"
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Please answer the user's question about the most recent state of the union address\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e2b86",
   "metadata": {
    "id": "7f7e2b86"
   },
   "outputs": [],
   "source": [
    "embeddings = HypotheticalDocumentEmbedder(\n",
    "    llm_chain=llm_chain, base_embeddings=base_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd83424",
   "metadata": {
    "id": "6dd83424"
   },
   "outputs": [],
   "source": [
    "result = embeddings.embed_query(\n",
    "    \"What did the president say about Ketanji Brown Jackson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31388123",
   "metadata": {
    "id": "31388123"
   },
   "source": [
    "## Using HyDE\n",
    "Now that we have HyDE, we can use it as we would any other embedding class! Here is using it to find similar passages in the state of the union example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97719b29",
   "metadata": {
    "id": "97719b29"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "with open(\"../../state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcfc039",
   "metadata": {
    "id": "bfcfc039"
   },
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_texts(texts, embeddings)\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632af7f2",
   "metadata": {
    "id": "632af7f2"
   },
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03d736",
   "metadata": {
    "id": "4f03d736"
   },
   "source": [
    "## Overall workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e57b93",
   "metadata": {
    "id": "b9e57b93"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document genration\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dedfdb",
   "metadata": {
    "id": "61dedfdb"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf4c50",
   "metadata": {
    "id": "aedf4c50"
   },
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1b4f8",
   "metadata": {
    "id": "57b1b4f8"
   },
   "outputs": [],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0be5e164895c4308bcbdd6c3d301c4d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10c59dd368ce478f95705af1df1776af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17e9c58c45434cb0a6cc70d188ec5cd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61ca35879b714b7cbd228a74ea628272": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8dc1a571cb4a4bdc9a06272be0ff8680",
      "placeholder": "​",
      "style": "IPY_MODEL_e541de6bc68d4f17a1d758305a60d0ed",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "6a44a02e8c094960b079f19d63b4128d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10c59dd368ce478f95705af1df1776af",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a7bfd34bdf1946e79503265d27cfb264",
      "value": 4
     }
    },
    "7bbd3a944f83430cb8f23c9715b86d4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dc1a571cb4a4bdc9a06272be0ff8680": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7bfd34bdf1946e79503265d27cfb264": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d97186eb647b4b4ea6632d52aad60dd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_61ca35879b714b7cbd228a74ea628272",
       "IPY_MODEL_6a44a02e8c094960b079f19d63b4128d",
       "IPY_MODEL_e8a64cd701ca49d494f3961750be71c0"
      ],
      "layout": "IPY_MODEL_0be5e164895c4308bcbdd6c3d301c4d7"
     }
    },
    "e541de6bc68d4f17a1d758305a60d0ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8a64cd701ca49d494f3961750be71c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bbd3a944f83430cb8f23c9715b86d4d",
      "placeholder": "​",
      "style": "IPY_MODEL_17e9c58c45434cb0a6cc70d188ec5cd6",
      "value": " 4/4 [02:33&lt;00:00, 33.80s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
