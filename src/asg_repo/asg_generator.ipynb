{"cells":[{"cell_type":"markdown","source":["## RAG工作流（7.14）\n","\n","0. 用户上传多个pdf，经过indexing以多个collection的形式存入chroma\n","\n","1. 用户提出问题，如“paper中都使用了什么方法？”。\n","\n","2. （可能不必要） 利用prompt engineering，通过LlaMa-3-8B将用户提出的问题rephrase，使之标准，易于大模型理解。\n","\n","4. 根据用户的问题，从数据库中检索与问题相关的文档片段，作为context。\n","\n","6. 使用检索到的context和用户的问题作为参数，输入到LlaMa-3-8B中生成最终回答，其中编写prompt engineering，提供详细且准确的信息。"],"metadata":{"id":"aWW2-ZuB1APf"},"id":"aWW2-ZuB1APf"},{"cell_type":"markdown","source":["## HyDE工作流\n","\n","0. 用户上传多个pdf，经过indexing以多个collection的形式存入chroma。\n","\n","1. 用户提出问题，如“paper中都使用了什么方法？”。\n","\n","2. （可能不必要）利用prompt engineering，通过LlaMa-3-8B将用户提出的问题rephrase，使之标准，易于大模型理解。\n","\n","3. 使用LlaMa-3-8B（prompt engineering）生成一个假设文档，这个文档对提出的问题进行补充描述，从不同角度提供信息。例如，生成文档可能会提供一个上下文回答，关于写paper时有哪些经常使用的方法。\n","\n","4. 使用HuggingFace的embedding模型（sentence-transformer）将生成的假设文档进行文本嵌入。将生成的嵌入存储到Chroma数据库中。\n","\n","5. 根据用户的问题，从数据库中检索与问题相关的文档片段，作为context。\n","\n","\n","6. 使用检索到的context（context是否要结合虚拟文档作为上下文？）和用户的问题作为参数，输入到LlaMa-3-8B中生成最终回答，其中编写prompt engineering，提供详细且准确的信息。\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"VTek6nzys_10"},"id":"VTek6nzys_10"},{"cell_type":"code","source":["import subprocess\n","\n","# 调用 nvidia-smi 命令\n","result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n","print(result.stdout.decode('utf-8'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfUahZpsd7lg","executionInfo":{"status":"ok","timestamp":1721167828159,"user_tz":420,"elapsed":392,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}},"outputId":"25d5def7-88d9-4962-f56f-b01a4cb81154"},"id":"sfUahZpsd7lg","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Jul 16 22:10:27 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0              46W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","id":"ccb74c9b","metadata":{"id":"ccb74c9b"},"source":["# HyDE\n","\n","Hypothetical Document Embeddings (HyDE) is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example, as described in [this paper](https://arxiv.org/abs/2212.10496).\n","\n","In order to use HyDE, we need to provide a base embedding model, as well as an LLMChain that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (the paper has more for details on them), but we can also create our own."]},{"cell_type":"code","source":["!pip install transformers\n","!pip install torch\n","!pip install langchain\n","!pip install langchain_community\n","!pip install langchain_core"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cbbwF0mubx5","executionInfo":{"status":"ok","timestamp":1721169108733,"user_tz":420,"elapsed":83339,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}},"outputId":"97fbadfa-b477-4e1f-b9c0-af53fd653081"},"id":"9cbbwF0mubx5","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n","Collecting langchain\n","  Downloading langchain-0.2.8-py3-none-any.whl (987 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.6/987.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-core<0.3.0,>=0.2.19 (from langchain)\n","  Downloading langchain_core-0.2.20-py3-none-any.whl (371 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n","  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.87-py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.19->langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.19->langchain) (24.1)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.19->langchain)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n","Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.8 langchain-core-0.2.20 langchain-text-splitters-0.2.2 langsmith-0.1.87 orjson-3.10.6\n","Collecting langchain_community\n","  Downloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Requirement already satisfied: langchain<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.8)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.20)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.87)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain_community) (0.2.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain_community) (2.8.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain_community) (24.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.7.4)\n","Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain_community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain_community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain_community) (2.20.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n","Successfully installed dataclasses-json-0.6.7 langchain_community-0.2.7 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n","Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.20)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.87)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.8.2)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.5.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (3.10.6)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.31.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2024.7.4)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JjgEiStquIcU","executionInfo":{"status":"ok","timestamp":1721167994008,"user_tz":420,"elapsed":18600,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}},"outputId":"f6b5d374-d524-4ce4-9315-f40bf4b066b4"},"id":"JjgEiStquIcU","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from langchain.prompts import PromptTemplate\n","import torch\n","import os"],"metadata":{"id":"oW4lY3PbYT1m","executionInfo":{"status":"ok","timestamp":1721168007824,"user_tz":420,"elapsed":8908,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}}},"id":"oW4lY3PbYT1m","execution_count":4,"outputs":[]},{"cell_type":"code","source":["# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","# from langchain.prompts import PromptTemplate\n","# import torch\n","# import os\n","# model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n","# if os.path.exists(model_path):\n","#     print(\"The path of Llama-3 exists.\")\n","# else:\n","#     print(\"The path of Llama-3 doesn't exist.\")\n","\n","# # load tokenizer and model\n","# tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n","# model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n","\n","# # let the the device be gpu (outside colab)\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model.to(device)\n","\n","\n","\n","class Generator:\n","    def __init__(self, model_path):\n","        self.model_path = model_path\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.tokenizer = None\n","        self.model = None\n","        self._load_model()\n","\n","    def _load_model(self):\n","        if os.path.exists(self.model_path):\n","            print(\"The path of Llama-3 exists.\")\n","        else:\n","            print(\"The path of Llama-3 doesn't exist.\")\n","\n","        # load tokenizer and model\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, local_files_only=True)\n","        self.model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True)\n","\n","        # let the device be gpu (outside colab)\n","        self.model.to(self.device)\n","\n","    def rephrase(self, question, rephrase_num, temp=0.7):\n","        \"\"\" Original version of rephrase function. \"\"\"\n","\n","        template = \"\"\"\n","        You are an assistant tasked with taking a natural language query from a user and converting it into a query for a vectorstore. In this process, you strip out information that is not relevant for the retrieval task. Here is the user query: {question}\n","        Rephrased Question: \"\"\"\n","\n","        prompt_template = PromptTemplate(template=template)\n","        result = [question]\n","        for i in range(rephrase_num):\n","            inputs = prompt_template.format(question=question)\n","            input_ids = self.tokenizer(inputs, return_tensors=\"pt\").input_ids.to(self.device)\n","\n","            outputs = self.model.generate(input_ids, max_length=100, temperature=temp, pad_token_id=self.tokenizer.eos_token_id)\n","            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","            # extract rephrased Question\n","            start_token = \"Rephrased Question:\"\n","            start_index = response.find(start_token) + len(start_token)\n","            if start_index != -1:\n","                rephrased_question = response[start_index:].strip().split('\\n')[0]\n","                if len(rephrased_question) > 1:\n","                    result.append(rephrased_question)\n","\n","        return result\n","\n","model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n","generator = Generator(model_path)\n","print(\"Successfully load the model.\")\n","\n","question_1 = \"What the hell are the methods used in the paper?\"\n","rephrased_question_1 = generator.rephrase(question_1, rephrase_num=1)\n","print(rephrased_question_1)\n","\n","question_2 = \"Can you tell me is the evaluation method used in article is 定性 or 定量?\"\n","rephrased_question_2 = generator.rephrase(question_2, rephrase_num=1)\n","print(rephrased_question_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["d97186eb647b4b4ea6632d52aad60dd7","61ca35879b714b7cbd228a74ea628272","6a44a02e8c094960b079f19d63b4128d","e8a64cd701ca49d494f3961750be71c0","0be5e164895c4308bcbdd6c3d301c4d7","8dc1a571cb4a4bdc9a06272be0ff8680","e541de6bc68d4f17a1d758305a60d0ed","10c59dd368ce478f95705af1df1776af","a7bfd34bdf1946e79503265d27cfb264","7bbd3a944f83430cb8f23c9715b86d4d","17e9c58c45434cb0a6cc70d188ec5cd6"]},"id":"PoB7CAQFgh4T","executionInfo":{"status":"ok","timestamp":1720978592514,"user_tz":420,"elapsed":174852,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}},"outputId":"19f78be4-25d1-4bd3-d3df-d8d28b6df885"},"id":"PoB7CAQFgh4T","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["The path of Llama-3 exists.\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d97186eb647b4b4ea6632d52aad60dd7"}},"metadata":{}}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from langchain.prompts import PromptTemplate\n","import torch\n","import os\n","\n","model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n","\n","def load_model():\n","    if os.path.exists(model_path):\n","        print(\"The path of Llama-3 exists.\")\n","    else:\n","        print(\"The path of Llama-3 doesn't exist.\")\n","\n","    # load tokenizer and model\n","    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n","    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n","\n","    # let the device be gpu (outside colab)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    return tokenizer, model, device\n","\n","tokenizer, model, device = load_model()\n","\n","def rephrase(question, rephrase_num, temp=0.7):\n","    \"\"\" Original version of rephrase function. \"\"\"\n","\n","    template = \"\"\"\n","    You are an assistant tasked with taking a natural language query from a user and converting it into a query for a vectorstore. In this process, you strip out information that is not relevant for the retrieval task. Here is the user query: {question}\n","    Rephrased Question: \"\"\"\n","\n","    prompt_template = PromptTemplate(template=template)\n","    result = [question]\n","    for i in range(rephrase_num):\n","        inputs = prompt_template.format(question=question)\n","        input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids.to(device)\n","\n","        outputs = model.generate(input_ids, max_length=100, temperature=temp, pad_token_id=tokenizer.eos_token_id)\n","        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # extract rephrased Question\n","        start_token = \"Rephrased Question:\"\n","        start_index = response.find(start_token) + len(start_token)\n","        if start_index != -1:\n","            rephrased_question = response[start_index:].strip().split('\\n')[0]\n","            if len(rephrased_question) > 1:\n","                result.append(rephrased_question)\n","\n","    return result\n","\n","print(\"Successfully load the model.\")\n","\n","question_1 = \"What the hell are the methods used in the paper?\"\n","rephrased_question_1 = rephrase(question_1, rephrase_num=1)\n","print(rephrased_question_1)\n","\n","question_2 = \"Can you tell me is the evaluation method used in article is 定性 or 定量?\"\n","rephrased_question_2 = rephrase(question_2, rephrase_num=2)\n","print(rephrased_question_2)\n"],"metadata":{"id":"ICHiD8ozk306"},"id":"ICHiD8ozk306","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def rephrase(question, rephrase_num, temp=0.7):\n","    \"\"\" Original version of rephrase function. \"\"\"\n","\n","    # HyDE requires hypothesis document, what's the format of the prompt?\n","    # below is the default prompt used in the from_llm classmethod\n","    template = \"\"\"\n","    You are an assistant tasked with taking a natural language query from a user and converting it into a query for a vectorstore.\n","    In this process, you strip out information that is not relevant for the retrieval task. Here is the user query: {question}\n","    Rephrased Question: \"\"\"\n","\n","    prompt_template = PromptTemplate(template=template)\n","\n","    result = [question]\n","    for i in range(rephrase_num):\n","        inputs = prompt_template.format(question=question)\n","        input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids.to(device)\n","\n","        outputs = model.generate(input_ids, max_length=100, temperature=temp, pad_token_id=tokenizer.eos_token_id)\n","        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","        # extract rephrased Question\n","        start_token = \"Rephrased Question:\"\n","        start_index = response.find(start_token) + len(start_token)\n","\n","        if start_index != -1:\n","            rephrased_question = response[start_index:].strip().split('\\n')[0]\n","            if len(rephrased_question) > 1:\n","                result.append(rephrased_question)\n","\n","    return result\n","\n","question_1 = \"What the hell are the methods used isssn the paper?\"\n","rephrased_question_1 = rephrase(question_1, rephrase_num=1)\n","print(rephrased_question_1)\n","question_2 = \"Can you tell me is the the evaluation method used in article is 定性 or 定量?\"\n","rephrased_question_2 = rephrase(question_2, rephrase_num=1)\n","print(rephrased_question_2)\n"],"metadata":{"id":"tWQ88BzDnlFD"},"id":"tWQ88BzDnlFD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n","\n","# need so many to download\n","# from asg_splitter import TextSplitting\n","# from asg_retriever import Retriever"],"metadata":{"id":"6Q8foPREwPME"},"id":"6Q8foPREwPME","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(context, question, temp=0.7):\n","\n","    model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n","    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    pipe = pipeline(\n","        \"text-generation\",\n","        model=model,\n","        tokenizer=tokenizer,\n","        device=0 if torch.cuda.is_available() else -1,\n","        batch_size=1,\n","        max_new_tokens=200,\n","        num_beams=4,\n","        do_sample=True,\n","        top_p=0.8,\n","        temperature=temp,\n","        repetition_penalty=1.5\n","    )\n","\n","    template = \"\"\"Use the following pieces of context to answer the question at the end.\n","    Provide a precise answer based on the context and attach the source coordinate SC of your answer in [SC]:\n","    ```\n","    ============================================================\n","    @3603a49a-d26c-4a2d-a9f1-0a608211b3ba//Verun is a current year-3 student of CS//\n","    @7628b67b-3eb1-437c-b302-f0df15847605//Verun is a boy//\n","    @fbb63230-714f-4b53-bfcc-20ca1f4ff734//Verun is from San Diego//\n","    END OF RESULT//\n","    ============================================================\n","\n","    Question: Who is Verun?\n","    ============================================================\n","    ```\n","    Think and respond with [@SC] in several complete and logical sentences:\n","    ```\n","    THOUGHT: The question ~~Who is Verun?~~ is asking for Verun's information.\n","\n","    ANSWER: Verun is a student from San Diego[SC: @fbb63230-714f-4b53-bfcc-20ca1f4ff734] and currently in CS[SC: @3603a49a-d26c-4a2d-a9f1-0a608211b3ba].\n","\n","    THOUGHT: 'Verun is a student from San Diego[SC: @fbb63230-714f-4b53-bfcc-20ca1f4ff734]' is not related to '@7628b67b-3eb1-437c-b302-f0df15847605//HAER is a boy//'.\n","\n","    FINAL ANSWER: As far as I know, Verun is a student from San Diego[SC: @e956dd] and currently in CS[SC: @0a3f01].\n","    ```\n","\n","    Now do the real task below!\n","\n","    ============================================================\n","    {context}\n","    ============================================================\n","    Question: {question}\n","    ============================================================\n","\n","    \"\"\"\n","\n","    prompt_template = PromptTemplate.from_template(template)\n","    formatted_prompt = prompt_template.format(context=context, question=question)\n","\n","    # generate answer using pipe\n","    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n","    outputs = model.generate(inputs, max_length=1000, temperature=temp)\n","    res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract final answer directly in the generate function\n","    final_answer_start = \"FINAL ANSWER:\"\n","    start_index = res.find(final_answer_start)\n","    if start_index != -1:\n","        final_answer = res[start_index + len(final_answer_start):].strip()\n","    else:\n","        final_answer = \"No final answer found.\"\n","\n","    return final_answer\n","\n","\n","context = \"\"\"\n","@3603a49a-d26c-4a2d-a9f1-0a608211b3ba//Verun is a current year-3 student of CS//\n","@7628b67b-3eb1-437c-b302-f0df15847605//Verun is a boy//\n","@fbb63230-714f-4b53-bfcc-20ca1f4ff734//Verun is from San Diego//\n","END OF RESULT//\n","\"\"\"\n","\n","question = \"Who is Verun?\"\n","\n","result = generate(context, question)\n","print(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"id":"Jzq2YGD7vBVr","executionInfo":{"status":"error","timestamp":1721168442159,"user_tz":420,"elapsed":425,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}},"outputId":"4f69bef9-f0bd-480b-bd25-f99e55cdc9ca"},"id":"Jzq2YGD7vBVr","execution_count":7,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tokenizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-2f8f11879a4a>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Who is Verun?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-2f8f11879a4a>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(context, question, temp)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# generate answer using pipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from langchain.prompts import PromptTemplate\n","\n","# 全局加载模型和分词器\n","model_path = \"/content/drive/MyDrive/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n","model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","def generate(context, question, temp=0.7):\n","    pipe = pipeline(\n","        \"text-generation\",\n","        model=model,\n","        tokenizer=tokenizer,\n","        device=0 if torch.cuda.is_available() else -1,\n","        batch_size=1,\n","        max_new_tokens=200,\n","        num_beams=4,\n","        do_sample=True,\n","        top_p=0.8,\n","        temperature=temp,\n","        repetition_penalty=1.5\n","    )\n","\n","    template = \"\"\"Use the following pieces of context to answer the question at the end.\n","    Provide a precise answer based on the context and attach the source coordinate SC of your answer in [SC]:\n","    ```\n","    ============================================================\n","    @3603a49a-d26c-4a2d-a9f1-0a608211b3ba//Verun is a current year-3 student of CS//\n","    @7628b67b-3eb1-437c-b302-f0df15847605//Verun is a boy//\n","    @fbb63230-714f-4b53-bfcc-20ca1f4ff734//Verun is from San Diego//\n","    END OF RESULT//\n","    ============================================================\n","\n","    Question: Who is Verun?\n","    ============================================================\n","    ```\n","    Think and respond with [@SC] in several complete and logical sentences:\n","    ```\n","    THOUGHT: The question ~~Who is Verun?~~ is asking for Verun's information.\n","\n","    ANSWER: Verun is a student from San Diego[SC: @fbb63230-714f-4b53-bfcc-20ca1f4ff734] and currently in CS[SC: @3603a49a-d26c-4a2d-a9f1-0a608211b3ba].\n","\n","    THOUGHT: 'Verun is a student from San Diego[SC: @fbb63230-714f-4b53-bfcc-20ca1f4ff734]' is not related to '@7628b67b-3eb1-437c-b302-f0df15847605//HAER is a boy//'.\n","\n","    FINAL ANSWER: As far as I know, Verun is a student from San Diego[SC: @e956dd] and currently in CS[SC: @0a3f01].\n","    ```\n","\n","    Now do the real task below!\n","\n","    ============================================================\n","    {context}\n","    ============================================================\n","    Question: {question}\n","    ============================================================\n","\n","    \"\"\"\n","\n","    prompt_template = PromptTemplate.from_template(template)\n","    formatted_prompt = prompt_template.format(context=context, question=question)\n","\n","    # generate answer using pipe\n","    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n","    outputs = model.generate(inputs, max_length=300, temperature=temp)\n","    res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract final answer directly in the generate function\n","    final_answer_start = \"FINAL ANSWER:\"\n","    start_index = res.find(final_answer_start)\n","    if start_index != -1:\n","        final_answer = res[start_index + len(final_answer_start):].strip()\n","    else:\n","        final_answer = \"No final answer found.\"\n","\n","    return final_answer\n","\n","context = \"\"\"\n","@3603a49a-d26c-4a2d-a9f1-0a608211b3ba//Verun is a current year-3 student of CS//\n","@7628b67b-3eb1-437c-b302-f0df15847605//Verun is a boy//\n","@fbb63230-714f-4b53-bfcc-20ca1f4ff734//Verun is from San Diego//\n","END OF RESULT//\n","\"\"\"\n","\n","question = \"Who is Verun?\"\n","\n","result = generate(context, question)\n","print(result)\n"],"metadata":{"id":"HhrqZcB_nFOT"},"id":"HhrqZcB_nFOT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n"],"metadata":{"id":"Tjo5KQrjTK7D","executionInfo":{"status":"ok","timestamp":1720979584543,"user_tz":420,"elapsed":132,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}}},"id":"Tjo5KQrjTK7D","execution_count":6,"outputs":[]},{"cell_type":"code","source":["template = \"\"\"Use the following pieces of context to answer the question at the end.\n","Provide only a precise answer based on the context and attach the source coordinate SC of your answer in [SC]:\n","# Answer with source [SC]:\n","{context}\n","Question: {question}\n","Think and respond with [@SC] in several complete and logical sentences.\n","Answer:\n","\"\"\"\n","\n","prompt_template = PromptTemplate.from_template(template)\n","\n","# an simple example\n","context = \"Shuki asked Verun if he is from San Diego. He said yes. \\\n","    Ayden is a current year 3 student in the department of COMP at POLYU. \\\n","    Verun is a roommate of Ayden. \\\n","    He is a current year 3 student in the department of CS at Stanford University.\"\n","question = \"Who is Verun?\"\n","\n","formatted_prompt = prompt_template.format(context=context, question=question)\n","\n","# generate the answer\n","inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n","outputs = model.generate(inputs, max_length=300, temperature=0.7)\n","res = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_YwX9v4JTs37","executionInfo":{"status":"ok","timestamp":1720661936288,"user_tz":420,"elapsed":6213,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}},"outputId":"4033632b-7d7a-40f4-b57f-8441bb9aa7b5"},"id":"_YwX9v4JTs37","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Use the following pieces of context to answer the question at the end. \n","Provide only a precise answer based on the context and attach the source coordinate SC of your answer in [SC]:\n","# Answer with source [SC]:\n","Shuki asked Verun if he is from San Diego. He said yes.     Ayden is a current year 3 student in the department of COMP at POLYU.     Verun is a roommate of Ayden.     He is a current year 3 student in the department of CS at Stanford University.\n","Question: Who is Verun?\n","Think and respond with [@SC] in several complete and logical sentences. \n","Answer:\n","Verun is a current year 3 student in the department of CS at Stanford University. He is also a roommate of Ayden, who is a current year 3 student in the department of COMP at POLYU. Additionally, Verun confirmed that he is from San Diego when Shuki asked him. [@SC] \n","Source Coordinate: [SC: 1] \n","Note: SC refers to the source coordinate of the answer, which is the number of the sentence in the original text where the information is found. In this case, the answer is based on sentences 1, 3, and 4.\n"]}]},{"cell_type":"code","source":["# HyDE prompt example\n","template = \"\"\"\n","Please write an introductory passage to answer the question:\n","Question: {question}\n","Passage:\n","\"\"\"\n","\n","# define the virtual document generation Prompt template\n","prompt_hyde_template = template\n","\n","# example question\n","question = \"Where is Hong Kong?\"\n","\n","# generate virtual document\n","formatted_prompt = prompt_hyde_template.format(question=question)\n","inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n","outputs = model.generate(inputs, max_length=300, temperature=0.7)\n","virtual_document = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","print(virtual_document)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gte5nFyFTsyX","executionInfo":{"status":"ok","timestamp":1720671778246,"user_tz":420,"elapsed":11301,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}},"outputId":"aa933f9d-beaf-447a-bad7-9f7f96e8bb5e"},"id":"Gte5nFyFTsyX","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Please write an introductory passage to answer the question:\n","Question: Where is Hong Kong?\n","Passage:\n","Hong Kong is a Special Administrative Region of China, located on the southeastern coast of the country. It is situated on the Pearl River Delta, which is one of the most densely populated regions in the world. Hong Kong is an archipelago, consisting of over 260 islands, with the largest being Hong Kong Island, Lantau Island, and the Kowloon Peninsula. The region is bordered by the Guangdong Province of China to the north, and the South China Sea to the south, east, and west. Hong Kong is a major financial and trade center, and its unique blend of East and West cultures has made it a popular tourist destination.\n","\n","Please note that the passage should be around 150-200 words. \n","\n","Here is the passage:\n","\n","Hong Kong is a Special Administrative Region of China, located on the southeastern coast of the country. It is situated on the Pearl River Delta, which is one of the most densely populated regions in the world. Hong Kong is an archipelago, consisting of over 260 islands, with the largest being Hong Kong Island, Lantau Island, and the Kowloon Peninsula. The region is bordered by the Guangdong Province of China to the north, and the South China Sea to the south, east, and west. Hong Kong is a major financial and trade center, and its unique blend of East and West cultures has made\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ikX-glCU3F2P"},"id":"ikX-glCU3F2P","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oqDsVfUs3Fok"},"id":"oqDsVfUs3Fok","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1XLf7P-w3FbC"},"id":"1XLf7P-w3FbC","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rOCj2FDT3FXq"},"id":"rOCj2FDT3FXq","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bW6BRLlW3FUY"},"id":"bW6BRLlW3FUY","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D21jFUDE3FRS"},"id":"D21jFUDE3FRS","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"546e87ee","metadata":{"id":"546e87ee"},"outputs":[],"source":["from langchain.chains import HypotheticalDocumentEmbedder, LLMChain\n","from langchain.prompts import PromptTemplate\n","from langchain_openai import OpenAI, OpenAIEmbeddings"]},{"cell_type":"code","source":["class HypotheticalDocumentEmbedder:\n","    def __init__(self, model, tokenizer):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","\n","    def embed_query(self, query):\n","        inputs = self.tokenizer(query, return_tensors=\"pt\").to(device)\n","        outputs = self.model(**inputs, output_hidden_states=True)\n","        embeddings = outputs.hidden_states[-1][:, 0, :].squeeze().detach().cpu().numpy()\n","        return embeddings"],"metadata":{"id":"A8DvNuZ7lvXf"},"id":"A8DvNuZ7lvXf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 实例化嵌入类\n","embedder = HypotheticalDocumentEmbedder(model, tokenizer)\n","\n","# 嵌入查询示例\n","query = \"Where is the Taj Mahal?\"\n","embedding = embedder.embed_query(query)\n","print(f\"Embedding for '{query}': {embedding}\")\n","print(len(embedding)) # 4096维"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXDoCTG2lyHj","executionInfo":{"status":"ok","timestamp":1720597340440,"user_tz":420,"elapsed":231,"user":{"displayName":"Zian WANG","userId":"02436955510043631970"}},"outputId":"af11fc5d-1e22-4dc5-b29b-ce017da64135"},"id":"HXDoCTG2lyHj","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding for 'Where is the Taj Mahal?': [ 4.185107   -0.20592627 -1.8382323  ... -2.890834    1.3604962\n","  0.31094578]\n","4096\n"]}]},{"cell_type":"code","execution_count":null,"id":"c0ea895f","metadata":{"id":"c0ea895f"},"outputs":[],"source":["base_embeddings = OpenAIEmbeddings()\n","llm = OpenAI()"]},{"cell_type":"code","execution_count":null,"id":"50729989","metadata":{"id":"50729989"},"outputs":[],"source":["# Load with `web_search` prompt\n","embeddings = HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, \"web_search\")"]},{"cell_type":"code","execution_count":null,"id":"3aa573d6","metadata":{"id":"3aa573d6"},"outputs":[],"source":["# Now we can use it as any embedding class!\n","result = embeddings.embed_query(\"Where is the Taj Mahal?\")"]},{"cell_type":"markdown","id":"1da90437","metadata":{"id":"1da90437"},"source":["## Using our own prompts\n","Besides using preconfigured prompts, we can also easily construct our own prompts and use those in the LLMChain that is generating the documents. This can be useful if we know the domain our queries will be in, as we can condition the prompt to generate text more similar to that.\n","\n","In the example below, let's condition it to generate text about a state of the union address (because we will use that in the next example)."]},{"cell_type":"code","execution_count":null,"id":"0b4a650f","metadata":{"id":"0b4a650f"},"outputs":[],"source":["prompt_template = \"\"\"Please answer the user's question about the most recent state of the union address\n","Question: {question}\n","Answer:\"\"\"\n","prompt = PromptTemplate(input_variables=[\"question\"], template=prompt_template)\n","llm_chain = LLMChain(llm=llm, prompt=prompt)"]},{"cell_type":"code","execution_count":null,"id":"7f7e2b86","metadata":{"id":"7f7e2b86"},"outputs":[],"source":["embeddings = HypotheticalDocumentEmbedder(\n","    llm_chain=llm_chain, base_embeddings=base_embeddings\n",")"]},{"cell_type":"code","execution_count":null,"id":"6dd83424","metadata":{"id":"6dd83424"},"outputs":[],"source":["result = embeddings.embed_query(\n","    \"What did the president say about Ketanji Brown Jackson\"\n",")"]},{"cell_type":"markdown","id":"31388123","metadata":{"id":"31388123"},"source":["## Using HyDE\n","Now that we have HyDE, we can use it as we would any other embedding class! Here is using it to find similar passages in the state of the union example."]},{"cell_type":"code","execution_count":null,"id":"97719b29","metadata":{"id":"97719b29"},"outputs":[],"source":["from langchain_community.vectorstores import Chroma\n","from langchain_text_splitters import CharacterTextSplitter\n","\n","with open(\"../../state_of_the_union.txt\") as f:\n","    state_of_the_union = f.read()\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","texts = text_splitter.split_text(state_of_the_union)"]},{"cell_type":"code","execution_count":null,"id":"bfcfc039","metadata":{"id":"bfcfc039"},"outputs":[],"source":["docsearch = Chroma.from_texts(texts, embeddings)\n","\n","query = \"What did the president say about Ketanji Brown Jackson\"\n","docs = docsearch.similarity_search(query)"]},{"cell_type":"code","execution_count":null,"id":"632af7f2","metadata":{"id":"632af7f2"},"outputs":[],"source":["print(docs[0].page_content)"]},{"cell_type":"markdown","id":"4f03d736","metadata":{"id":"4f03d736"},"source":["## Overall workflow"]},{"cell_type":"code","execution_count":null,"id":"b9e57b93","metadata":{"id":"b9e57b93"},"outputs":[],"source":["from langchain.prompts import ChatPromptTemplate\n","\n","# HyDE document genration\n","template = \"\"\"Please write a scientific paper passage to answer the question\n","Question: {question}\n","Passage:\"\"\"\n","prompt_hyde = ChatPromptTemplate.from_template(template)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"61dedfdb","metadata":{"id":"61dedfdb"},"outputs":[],"source":["from langchain_core.output_parsers import StrOutputParser\n","from langchain_openai import ChatOpenAI\n","\n","generate_docs_for_retrieval = (\n","    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser()\n",")\n","\n","# Run\n","question = \"What is task decomposition for LLM agents?\"\n","generate_docs_for_retrieval.invoke({\"question\":question})"]},{"cell_type":"code","execution_count":null,"id":"aedf4c50","metadata":{"id":"aedf4c50"},"outputs":[],"source":["# Retrieve\n","retrieval_chain = generate_docs_for_retrieval | retriever\n","retireved_docs = retrieval_chain.invoke({\"question\":question})\n","retireved_docs"]},{"cell_type":"code","execution_count":null,"id":"57b1b4f8","metadata":{"id":"57b1b4f8"},"outputs":[],"source":["# RAG\n","template = \"\"\"Answer the following question based on this context:\n","\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","final_rag_chain = (\n","    prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d97186eb647b4b4ea6632d52aad60dd7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61ca35879b714b7cbd228a74ea628272","IPY_MODEL_6a44a02e8c094960b079f19d63b4128d","IPY_MODEL_e8a64cd701ca49d494f3961750be71c0"],"layout":"IPY_MODEL_0be5e164895c4308bcbdd6c3d301c4d7"}},"61ca35879b714b7cbd228a74ea628272":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dc1a571cb4a4bdc9a06272be0ff8680","placeholder":"​","style":"IPY_MODEL_e541de6bc68d4f17a1d758305a60d0ed","value":"Loading checkpoint shards: 100%"}},"6a44a02e8c094960b079f19d63b4128d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10c59dd368ce478f95705af1df1776af","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7bfd34bdf1946e79503265d27cfb264","value":4}},"e8a64cd701ca49d494f3961750be71c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bbd3a944f83430cb8f23c9715b86d4d","placeholder":"​","style":"IPY_MODEL_17e9c58c45434cb0a6cc70d188ec5cd6","value":" 4/4 [02:33&lt;00:00, 33.80s/it]"}},"0be5e164895c4308bcbdd6c3d301c4d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dc1a571cb4a4bdc9a06272be0ff8680":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e541de6bc68d4f17a1d758305a60d0ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10c59dd368ce478f95705af1df1776af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7bfd34bdf1946e79503265d27cfb264":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bbd3a944f83430cb8f23c9715b86d4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17e9c58c45434cb0a6cc70d188ec5cd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}