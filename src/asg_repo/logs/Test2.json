[
    {
        "chunk_id": "47c838a0-c733-4244-a19b-5487b537aa34",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "ABSTRACT High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is"
    },
    {
        "chunk_id": "996c93cc-c52b-47df-8a2b-18eeddbbf50a",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called"
    },
    {
        "chunk_id": "b4774637-835f-472f-a574-592eb3da3e21",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "AnglE. The core idea of AnglE is to introduce angle optimization in a complex"
    },
    {
        "chunk_id": "1efd3f41-5f64-4612-94c0-d309b3920774",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on"
    },
    {
        "chunk_id": "d797f6f6-6d3f-49c9-ae61-21df6c75c191",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with"
    },
    {
        "chunk_id": "3c71c746-bc98-4084-89ea-d3e4781b7176",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "limited labeled data and explore how AnglE works with LLM-annotated data. Extensive experiments were conducted on various tasks including short-text STS,"
    },
    {
        "chunk_id": "f8351eb3-4e7c-4e40-abe5-8e16e69bedc8",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "long-text STS, and domain-specific STS tasks. The results show that AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone. These findings demonstrate the ability of AnglE to generate high-quality"
    },
    {
        "chunk_id": "0fe943e1-57a6-4306-88dc-81c0acedbb81",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "text embeddings and the usefulness of angle optimization in STS. 1 I NTRODUCTION The development of text embeddings (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Cer et al., 2018; Reimers & Gurevych, 2019; Gao et al., 2021) is an essential research challenge in"
    },
    {
        "chunk_id": "1c708da1-7483-47a7-bea7-0b17b550ede2",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "the NLP community. Text embeddings effectively feature key semantic and syntactic information in language, which broadly affects the performance of downstream tasks, such as text classification (Li et al., 2021), sentiment analysis (Suresh & Ong, 2021; Zhang et al., 2022), semantic matching"
    },
    {
        "chunk_id": "30e72714-de6c-4d08-9757-aa109d0ef6db",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "(Grill et al., 2020; Lu et al., 2020), clustering (Reimers & Gurevych, 2019; Xu et al., 2023), and question-answering (QA) system (Yue et al., 2021). In particular, text embedding models play a crucial role in LLMs such as ChatGPT (OpenAI, 2022; 2023), LLaMA (Touvron et al., 2023a;b),"
    },
    {
        "chunk_id": "2e6bdcf8-629e-4aef-bc0c-ba634ad33b01",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "and ChatGLM (Du et al., 2022)-based applications. These LLM-based applications heavily rely on high-quality text embeddings for tasks such as vector search, where related documents are retrieved for LLM QA (Asai et al., 2023)."
    },
    {
        "chunk_id": "de14dcfe-a6c4-49da-a91d-84735fca2699",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "Recent studies (Gao et al., 2021; Jiang et al., 2022b; Chuang et al., 2022; Chanchani & Huang, 2023; Zhuo et al., 2023) have utilized pre-trained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in combination with contrastive learning to enhance the quality of text"
    },
    {
        "chunk_id": "4b7cbc4f-a68e-4830-b644-b5f599fa9773",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "embeddings. These approaches involve pulling semantically similar samples together and pushing apart those not (Gao et al., 2021). In these contrastive models, positive samples that are semantically similar can be generated by data augmentation, while negative samples that are dissimilar are"
    },
    {
        "chunk_id": "b63865be-a11e-4baf-a8c0-aec6aa8fd024",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "selected from different texts within the same mini-batch (in-batch negatives). However, supervised negatives are underutilized, and the correctness of in-batch negatives is difficult to guarantee without"
    },
    {
        "chunk_id": "0392c364-7173-43e7-9a22-b0a737a51e32",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "annotation, which can lead to performance degradation. Although some models such as (Gao et al., 2021) optimize hard negative samples, they rely on strict triple formats ( xi,x+ i,x\u2212 i). While most existing supervised STS datasets only provide pairs ( xi,x+ i) or (xj,x\u2212 j), where x+ irefers to the"
    },
    {
        "chunk_id": "ba22e8c6-a0ea-4e66-8569-5d82d491ca52",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "irefers to the positive sample of xiwhile x\u2212 jthe negative sample of xj. Thus, most contrastive models are used in unsupervised settings yet might not benefit from human supervision. \u2217Corresponding author 1arXiv:2309.12871v7  [cs.CL]  16 May 2024Preprint x cos(x)saturation zoney"
    },
    {
        "chunk_id": "8a79cf14-7762-4849-99d4-29e8573be1bd",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "zoney Figure 1: The saturation zones of the cosine function. The gradient at saturation zones is close to zero. During backpropagation, if the gradient is very small, it could kill the gradient and make the network difficult to learn."
    },
    {
        "chunk_id": "88f98fd6-6af5-44fe-9662-cabb9c04af4e",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "For supervised STS (Reimers & Gurevych, 2019; Su, 2022), most efforts to date employed the cosine function in their training objective to measure the pairwise semantic similarity. However, the cosine function has saturation zones, as shown in Figure 1. It can impede the optimization due"
    },
    {
        "chunk_id": "f2fc7597-1960-4b01-b93d-20f670171505",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "to the gradient vanishing issue and hinder the ability to learn subtle distinctions between texts in backpropagation. Additionally, many STS datasets such as MRPC1and QQP2provide binary labels representing dissimilar ( 0) and similar ( 1), which naturally fall within the saturation zone of"
    },
    {
        "chunk_id": "86557f53-77b4-4710-9349-e5fab0add951",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "the cosine function. To overcome this challenge, this paper proposes a novel angle-optimized text embedding. It optimizes not only the cosine similarity between texts but also the angle to mitigate the"
    },
    {
        "chunk_id": "6e07bbc2-a538-463b-b12f-7770bfdcd11a",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "negative impact of the saturation zones of the cosine function on the learning process. Specifically, it first divides the text embedding into real and imaginary parts in a complex space. Then, it follows"
    },
    {
        "chunk_id": "520b8523-fce3-4ad6-bc55-390ecbe769f1",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "the division rule in complex space to compute the angle difference between two text embeddings. After normalization, the angle difference becomes an objective to be optimized. It is intuitive to optimize the normalized angle difference, because if the normalized angle difference between two"
    },
    {
        "chunk_id": "0c814333-a9cc-4088-a041-a307dc34132a",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "text embeddings is smaller, it means that the two text embeddings are closer to each other in the complex space, i.e., their similarity is larger. In the STS experimental setup, we observed that the majority of existing STS benchmarks focus on"
    },
    {
        "chunk_id": "0e346243-45b8-4689-a88e-e260a39f2cd6",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "evaluating models on short texts. Unfortunately, there is a lack of datasets specifically designed to"
    },
    {
        "chunk_id": "1b1ff33c-c911-4842-9e35-30265d9d6303",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "evaluate the STS performance of models on long texts. Long texts are prevalent in real-world applications such as financial documents, legal documents, and health reports (Li et al., 2023). To tackle"
    },
    {
        "chunk_id": "b764aa8d-efe1-4aa5-ab25-e9b6e8882377",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "this challenge, this paper presents a new high-quality long-text STS dataset. This dataset allows for a more thorough evaluation of model performance on long texts. Specifically, the dataset is collected"
    },
    {
        "chunk_id": "f06219ef-8311-4fb3-af88-d7322cb959fd",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "from GitHub Issues with roughly 21K samples, we use the duplicate issues as the positive samples and the non-duplicate issues as the negative samples. We first experimented with both short and long-text datasets and showed that AnglE outperforms"
    },
    {
        "chunk_id": "6cb57937-819f-4b9d-abdb-08ae1cf8222c",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "the SOTA STS models in both transfer and non-transfer STS tasks. For example, AnglE shows an average Spearman correlation of 73.55% in non-transfer STS tasks, compared to 68.03% for SBERT. Then, an ablation study shows that all components contribute positively to AnglE\u2019s superior"
    },
    {
        "chunk_id": "92a7d405-16b8-478a-83a6-b5e119259a05",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "performance. Next, we discuss the domain-specific scenarios with limited annotated data that are challenging for AnglE-like supervised STS, where it is observed that AnglE can work well with LLM-supervised data. Finally, we find that AnglE can benefit downstream retrieval applications and"
    },
    {
        "chunk_id": "ccb66ea1-8d76-42ac-8e8c-869be4aed2c6",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "can learn representations closer to actual representations. In summary, the contributions of this paper are listed as follows: \u2022We investigate the negative effects of saturation zone in the cosine function widely applied in STS"
    },
    {
        "chunk_id": "26afc1eb-a8ce-47d8-bab7-fd12dd7fac81",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "and propose a novel angle-optimized text embedding model to mitigate this issue. \u2022We extend the existing STS benchmark with a newly collected long-text dataset from Github Issues to allow a more comprehensive empirical study in STS."
    },
    {
        "chunk_id": "75880307-b11e-4d41-9d15-8a35ddd62e4a",
        "metadata": {
            "doc_name": "Test2.pdf"
        },
        "page_content": "\u2022We present extensive experiments on STS and demonstrate that AnglE can substantially improve the text embedding quality in various scenarios. 1https://www.microsoft.com/en-us/download/details.aspx?id=52398 2https://www.quora.com/q/quoradata/ 2Preprint"
    }
]