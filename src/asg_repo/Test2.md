# A NGLE - OPTIMIZED  T EXT  E MBEDDINGS  

Xianming Li, Jing Li   ∗  

Department of Computing, The Hong Kong Polytechnic University, Hong Kong SAR xianming.li@connect.polyu.hk, jing-amelia.li@polyu.edu.hk  

# A BSTRACT  

High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) ap- plications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the satura- tion zone in the cosine function, which can impede gradient and hinder optimiza- tion processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works with LLM-annotated data. Ex- tensive experiments were conducted on various tasks including short-text STS, long-text STS, and domain-specific STS tasks. The results show that AnglE out- performs the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone. These findings demonstrate the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS.  

# 1 I NTRODUCTION  

The development of text embeddings (Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017; Cer et al., 2018; Reimers & Gurevych, 2019; Gao et al., 2021) is an essential research challenge in the NLP community. Text embeddings effectively feature key semantic and syntactic information in language, which broadly affects the performance of downstream tasks, such as text classification

 (Li et al., 2021), sentiment analysis (Suresh & Ong, 2021; Zhang et al., 2022), semantic matching

 (Grill et al., 2020; Lu et al., 2020), clustering (Reimers & Gurevych, 2019; Xu et al., 2023), and question-answering (QA) system (Yue et al., 2021). In particular, text embedding models play a crucial role in LLMs such as ChatGPT (OpenAI, 2022; 2023), LLaMA (Touvron et al., 2023a;b), and ChatGLM (Du et al., 2022)-based applications. These LLM-based applications heavily rely on high-quality text embeddings for tasks such as vector search, where related documents are retrieved for LLM QA (Asai et al., 2023).  

Recent studies (Gao et al., 2021; Jiang et al., 2022b; Chuang et al., 2022; Chanchani & Huang, 2023; Zhuo et al., 2023) have utilized pre-trained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in combination with contrastive learning to enhance the quality of text embeddings. These approaches involve pulling semantically similar samples together and pushing apart those not (Gao et al., 2021). In these contrastive models, positive samples that are semanti- cally similar can be generated by data augmentation, while negative samples that are dissimilar are selected from different texts within the same mini-batch (in-batch negatives). However, supervised negatives are underutilized, and the correctness of in-batch negatives is difficult to guarantee without annotation, which can lead to performance degradation. Although some models such as (Gao et al., 2021) optimize hard negative samples, they rely on strict triple formats  $(x_{i},\,x_{i}^{+},\,x_{i}^{-})$  ). While most existing supervised STS datasets only provide pairs   $(x_{i},\,x_{i}^{+})$  ) or   $(x_{j},\,x_{j}^{-})$  ), where    $\bar{x}_{i}^{+}$    refers to the positive sample of    $x_{i}$   while  $x_{j_{\l}}^{-}$    the negative sample of    $x_{j}$  . Thus, most contrastive models are used in unsupervised settings yet might not benefit from human supervision.  

![](images/7d65e5dab68a1790409bd0d83bd2ece19e5af1b68d445ec6421a6ac515255344.jpg)  
Figure 1: The saturation zones of the cosine function. The gradient at saturation zones is close to zero. During backpropagation, if the gradient is very small, it could kill the gradient and make the network difficult to learn.  

For supervised STS (Reimers & Gurevych, 2019; Su, 2022), most efforts to date employed the cosine function in their training objective to measure the pairwise semantic similarity. However, the cosine function has saturation zones, as shown in Figure 1. It can impede the optimization due to the gradient vanishing issue and hinder the ability to learn subtle distinctions between texts in backpropagation. Additionally, many STS datasets such as MRPC   1   and QQP   2   provide binary labels representing dissimilar ( 0 ) and similar ( 1 ), which naturally fall within the saturation zone of the cosine function. To overcome this challenge, this paper proposes a novel angle-optimized text embedding. It optimizes not only the cosine similarity between texts but also the angle to mitigate the negative impact of the saturation zones of the cosine function on the learning process. Specifically, it first divides the text embedding into real and imaginary parts in a complex space. Then, it follows the division rule in complex space to compute the angle difference between two text embeddings. After normalization, the angle difference becomes an objective to be optimized. It is intuitive to optimize the normalized angle difference, because if the normalized angle difference between two text embeddings is smaller, it means that the two text embeddings are closer to each other in the complex space, i.e., their similarity is larger.  

In the STS experimental setup, we observed that the majority of existing STS benchmarks focus on evaluating models on short texts. Unfortunately, there is a lack of datasets specifically designed to evaluate the STS performance of models on long texts. Long texts are prevalent in real-world appli- cations such as financial documents, legal documents, and health reports (Li et al., 2023). To tackle this challenge, this paper presents a new high-quality long-text STS dataset. This dataset allows for a more thorough evaluation of model performance on long texts. Specifically, the dataset is collected from GitHub Issues with roughly  21 K samples, we use the duplicate issues as the positive samples and the non-duplicate issues as the negative samples.  

We first experimented with both short and long-text datasets and showed that AnglE outperforms the SOTA STS models in both transfer and non-transfer STS tasks. For example, AnglE shows an average Spearman correlation of    $73.55\%$   in non-transfer STS tasks, compared to    $68.03\%$   for SBERT. Then, an ablation study shows that all components contribute positively to AnglE’s superior performance. Next, we discuss the domain-specific scenarios with limited annotated data that are challenging for AnglE-like supervised STS, where it is observed that AnglE can work well with LLM-supervised data. Finally, we find that AnglE can benefit downstream retrieval applications and can learn representations closer to actual representations.  

In summary, the contributions of this paper are listed as follows:  

•  We investigate the negative effects of saturation zone in the cosine function widely applied in STS and propose a novel angle-optimized text embedding model to mitigate this issue. •  We extend the existing STS benchmark with a newly collected long-text dataset from Github Issues to allow a more comprehensive empirical study in STS. •  We present extensive experiments on STS and demonstrate that AnglE can substantially improve the text embedding quality in various scenarios.  

# 2 R ELATED  W ORK  

This section is organized as follows: we first introduce the unsupervised approaches, then the super- vised approaches, and finally give a summary.  

Unsupervised Approaches Early studies (Hill et al., 2016; Pagliardini et al., 2018) have demon- strated the efficacy of augmenting word2vec (Mikolov et al., 2013) with n-gram embeddings, yield- ing strong results in text embeddings. Recently, BERT-flow (Li et al., 2020) has introduced a flow- based approach that maps BERT embeddings to a standard Gaussian latent space. On the other hand, BERT-whitening (Su et al., 2021) applies the whitening operation to BERT embeddings to enhance text embeddings. Furthermore, very recent research (Carlsson et al., 2020; Zhang et al., 2020; Giorgi et al., 2021; Gao et al., 2021; Yan et al., 2021; Chuang et al., 2022; Jiang et al., 2022b; Zhuo et al., 2023) has focused on leveraging contrastive objectives to improve the quality of text embeddings.  

Supervised Approaches Supervised text embeddings usually perform better than their unsuper- vised counterparts (Gao et al., 2021). Various studies have effectively utilized supervised datasets to enhance the learning of text embeddings. In particular, Conneau et al. (2017) introduced a method that leverages supervised Natural Language Inference (NLI) tasks for this purpose. Building on a transformer backbone, USE (Cer et al., 2018) incorporates the SNLI dataset to augment unsu- pervised training, resulting in improved performance. Furthermore, SBERT (Reimers & Gurevych, 2019) enhances text embedding by combining BERT with a siamese architecture. Jiang et al. (2022a; 2023) proposed the use of prompt engineering to improve text embeddings.  

However, most existing models optimize the cosine similarity but neglect the negative effect of the saturation zone of the cosine function. To address this issue, this paper proposes a novel angle- optimized text embedding model to improve the quality of text embedding.  

# 3 M ETHODOLOGY  

This section will introduce the components of the proposed angle-optimized text embedding model, including the input layer, cosine objective, in-batch negative objective, and angle objective.  

# 3.1 I NPUT  L AYER  

For the input sentences, we first apply padding to ensure a consistent length  $l$  . Next, we map each word to a continuous    $d$  -dimensional space to produce wor  $\mathbf{e}_{i}~\in~\mathbb{R}^{d}$  ese word embeddings are then concatenated to form the model input:  ${\bf E}=[{\bf e}_{1},\bar{{\bf e}_{2}},\bar{.}\,.\,.\,,{\bf e}_{l}]\in\mathbb{R}^{l\times d}$   ∈ . Subse- quently, the model input is passed through an encoder such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and LLaMA (Touvron et al., 2023a;b) to obtain the contextual representation    $\mathbf{X}$  .  

# 3.2 C OSINE  O BJECTIVE  

Following the prior study (Su, 2022), we employ the cosine objective function for end-to-end opti- mization of cosine similarity between representations, as follows:  

$$
\mathcal{L}_{c o s}=\log\left[1+\sum_{s(\mathbf{X}_{i},\mathbf{X}_{j})>s(\mathbf{X}_{m},\mathbf{X}_{n})}e^{\frac{\cos(\mathbf{X}_{m},\mathbf{X}_{n})-\cos(\mathbf{X}_{i},\mathbf{X}_{j})}{\tau}}\right]
$$  

where    $\tau$   is a tempe ture  perparameter,    $\cos(\cdot)$  e cosine similarity function, and    $s(u,v)$   is the similarity between  u  and  v . By optimizing the  L  $\mathcal{L}_{c o s}$  , we expect the cosine similarity of the high similarity pair to be greater than that of the low similarity pair.  

# 3.3 I N - BATCH  N EGATIVE  O BJECTIVE  

To further improve performance, we integrate the in-batch negative objective function. Because in-batch negative samples can serve as a data augmentation technique, which can benefit the gen- eralization. Unlike existing contrastive learning models (Gao et al., 2021; Yan et al., 2021) that generate positive samples through data augmentation, we use supervised positive samples. Recog- nizing that there might be identical sentences within a batch that are not explicitly labeled as positive samples, causing them to become in-batch negatives, we identify these duplicate sentences and as- sign them as positive samples, thereby reducing potential noise. The formulation for the in-batch negative objective function (ibn) is as follows:  

$$
\mathcal{L}_{i b n}=-\sum_{b}\sum_{i}^{m}\log\left[\frac{e^{\cos(\mathbf{X}_{b_{i}},\mathbf{X}_{b_{i}}^{+})/\tau}}{\sum_{j}^{N}e^{\cos(\mathbf{X}_{b_{i}},\mathbf{X}_{b_{j}}^{+})/\tau}}\right],
$$  

where  $\tau$   is a temperature hyperparameter,    $b$   stands for the  $b$  -th batch,  $\mathbf{X}_{b_{i}}^{+}$    and  $\mathbf{X}_{b_{j}}^{+}$    are the respective positive samples of  $\mathbf{X}_{b_{i}}$   and  $\mathbf{X}_{b_{j}}$  ,    $m$   represents the number of positive pairs in  $b$  -th batch,  $N$   is the batch size, and  $\cos(\cdot)$   is the cosine similarity function.  

![](images/413a83705ef968ec6652eb98cec55a5bcef56ca6ef7008d484c99503405f8949.jpg)  
Figure 2: (a) Division in complex space.  $\Delta\theta$   is the angle difference between dividend    $z$   and divisor  $w$  in complex space. (b) Angle optimization in cosine saturation zones. Even though    $\Delta y\approx0$   could kill the gradient, the corresponding angle difference in complex space is still distinct for optimization.  

# 3.4ANGLE OBJECTIVE  

We found that both the cosine and in-batch negative objectives employ the cosine function to mea- sure similarity. However, it is important to note that the cosine function includes saturation zones, which can hinder the optimization process. We optimize the angle difference in complex space to mitigate these adverse effects. Figure 2a draws the division in complex space, and Figure 2b de- picts how angle optimization works in cosine saturation zones. To optimize the angle difference, we define    $\mathbf{X}^{r e}$    and    $\bar{\mathbf{X}}^{i m}$    are the real part and the imaginary part of    $\mathbf{X}$  . We follow the implementation of (Sun et al., 2019) to obtain    $\mathbf{X}^{r e}$    and    $\mathbf{X}^{i m}$    by the chunking strategy. Specifically, for the pair  $(\mathbf{X}_{i},\mathbf{X}_{j})$  , their representations in the complex space are defined as follows:  

$$
\begin{array}{r}{\mathbf{z}=\mathbf{a}+\mathbf{b}i\in\mathbb{C}}\\ {\mathbf{w}=\mathbf{c}+\mathbf{d}i\in\mathbb{C},}\end{array}
$$  

where    $\mathbf{a}=\mathbf{X}_{i}^{r e}\in\mathbb{R},\,\mathbf{b}=\mathbf{X}_{i}^{i m}\in\mathbb{R},\,\mathbf{c}=\mathbf{X}_{j}^{r e}\in\mathbb{R}$  , and    $\mathbf{d}=\mathbf{X}_{j}^{i m}\in\mathbb{R}$  . To compute the angle difference between  z  and  w , we calculate division in complex space in polar coordinates, as follows:  

$$
\begin{array}{r l}&{\frac{\mathbf{z}}{\mathbf{w}}=\gamma\Delta\theta_{z w}}\\ &{\quad\quad\gamma=\frac{r_{\mathbf{z}}}{r_{\mathbf{w}}}=\frac{\sqrt{\mathbf{a}^{2}+\mathbf{b}^{2}}}{\sqrt{\mathbf{c}^{2}+\mathbf{d}^{2}}}}\\ &{\Delta\theta_{z w}=\theta_{\mathbf{z}}-\theta_{\mathbf{w}},}\end{array}
$$  

where  $r_{\mathbf{z}}$   and    $r_{\mathbf{w}}$   represent the magnitudes of    $\mathbf{z}$   and    $\mathbf{w}$  , while  $\theta_{\mathbf{z}}$   and  $\theta_{\mathbf{w}}$   denote the respective angles of    $\mathbf{z}$   and  w . Next, we compute the value of  $\frac{\mathbf{z}}{\mathbf{w}}$    by the division rule in complex space, as follows:  

$$
{\frac{\mathbf{z}}{\mathbf{w}}}={\frac{\mathbf{a}+\mathbf{b}i}{\mathbf{c}+\mathbf{d}i}}={\frac{(\mathbf{ac}+\mathbf{bd})+(\mathbf{bc}-\mathbf{ad})i}{\mathbf{c}^{2}+\mathbf{d}^{2}}}.
$$  

By employing Eq. 4 and Eq. 5, we can calculate the angle difference between  z  and  w  by multiply- ing both sides by  $\frac{1}{\gamma}$    , which can be seen as a normalization operation. In this paper, we determine the absolute normalized angle difference using the following expression:  

$$
\begin{array}{l}{\Delta\theta_{z w}=\mathrm{abs}({\frac{\mathbf{z}}{\mathbf{w}}}\times{\frac{1}{\gamma}})}\\ {\qquad=\mathrm{abs}\left[{\frac{(\mathbf{ac}+\mathbf{bd})+(\mathbf{bc}-\mathbf{ad})i}{\mathbf{c}^{2}+\mathbf{d}^{2}}}\times{\frac{\sqrt{\mathbf{c}^{2}+\mathbf{d}^{2}}}{\sqrt{\mathbf{a}^{2}+\mathbf{b}^{2}}}}\right]}\\ {\qquad=\mathrm{abs}\left[{\frac{(\mathbf{ac}+\mathbf{bd})+(\mathbf{bc}-\mathbf{ad})i}{\sqrt{(\mathbf{c}^{2}+\mathbf{d}^{2})(\mathbf{a}^{2}+\mathbf{b}^{2})}}}\right].}\end{array}
$$  

Then, the angle difference can be optimized by the following objective function:  

$$
\mathcal{L}_{a n g l e}=\log\left[1+\sum_{s(\mathbf{X}_{i},\mathbf{X}_{j})>s(\mathbf{X}_{m},\mathbf{X}_{n})}e^{\frac{\Delta\theta_{i j}-\Delta\theta_{m n}}{\tau}}\right]
$$  

where    $\tau$   is a temperature hyperparameter and    $s(u,v)$   is the similarity between    $u$   and    $v$  . By opti- mizing the    $\mathcal{L}_{a n g l e}$  , our objective is to minimize the normalized angle difference for pairs with high similarity compared to those with low similarity.  

Finally, we combine the aforementioned three objective functions in the following manner to form the final objective function:  

$$
\mathcal{L}=w_{1}*\mathcal{L}_{c o s}+w_{2}*\mathcal{L}_{i b n}+w_{3}*\mathcal{L}_{a n g l e},
$$  

where  $w_{1},w_{2}$  , and  $w_{3}$   are constants.  

# 4 E XPERIMENT  

# 4.1 D ATASETS AND  E VALUATION  M ETRICS  

Existing STS Benchmarks We mainly evaluate our model on several widely-adopted STS datasets, namely: MRPC, QQP, QNLI   3 , STS 2012-2016 (Agirre et al., 2012; 2013; 2014; 2015; 2016), SICK-R (Marelli et al., 2014), and STS-B (Cer et al., 2017). These datasets mainly consist of short text, but real-world scenarios often involve long text documents. Thus, we introduce a newly long-text dataset called  GitHub Issues Similarity Dataset  to comprehensively evaluate the STS task.  

GitHub Issues Similarity Dataset We observed the presence of many duplicate issues on GitHub. Typically, the maintainers of open source organizations tend to mark these duplicate issues as closed with a comment like “closing as a duplicate of #id”. Consequently, these duplicate issues inherently serve as a source of the STS task. It is also worth noting that most issues contain long texts because of the inclusion of extensive code within the issues. To compile the dataset, we extracted duplicated issues from  55  popular open-source projects (see A.1) on GitHub using GitHub API   4 . The dupli- cated issues were used as positive samples, while the remaining issues were considered negative samples. Table 1 presents statistics of the GitHub Issues Similarity Dataset, while Figure 3 shows a violin plot illustrating the token-level text length distribution. The visualization reveals a substantial number of long texts. Specifically, the proportion of long texts (token length  $>\,512$  ) for the train, validation, and test sets is at    $61.03\%$  ,    $60.85\%$  , and  $60.50\%$  , respectively.  

Evaluation Metrics To ensure a fair comparison, we follow previous studies and use Spearman’s correlation for evaluation. We use SentEval (Conneau & Kiela, 2018) to compute Spearman’s cor- relation and report the results in the “all” setting, which is consistent with the baselines.  

![](images/dff9cb9ebdb54906e6a4e2486a07f80af309bb436ff96010d77c904a1323e4fc.jpg)  
Figure 3: Log token length distribution of the GitHub Issue Similarity Dataset.  

Table 1: Statistics of the proposed GitHub Issues Similar- ity Dataset. #Positive denotes the count of positive pairs, and #Negative represents the number of negative pairs.  

![](images/010952a9a38f1c06100b7cf39e7221634673c6be13367e6afff2655c5563eabd.jpg)  

# 4.2 I MPLEMENTATION  D ETAILS  

In this paper, we use the pre-trained uncased BERT base model (110M parameters) as the backbone model. For a fair comparison, all BERT-based baselines also adopt this setting. We set the value of    $\tau$   for the cosine objective and the in-batch negative objective to  0 . 05 , based on prior research. Additionally, we determined the value of  $\tau$   for the angle objective to be  1 . 0  through grid search.  

# 4.3 M AIN  R ESULTS  

In this section, we will first introduce the baselines, then the results of the transfer STS tasks, then the results of the non-transfer STS tasks, and finally a summary.  

Baselines We compare our proposed model with widely used baselines, encompassing both un- supervised and supervised models. The unsupervised models are average GloVe (Pennington et al., 2014), BERT-flow (Li et al., 2020), BERT-whitening (Su et al., 2021), LLaMA2 (Touvron et al., 2023b), and contrastive learning models including IS-BERT (Zhang et al., 2020), CT-BERT (Carls- son et al., 2020), SimCSE (Gao et al., 2021), ConSERT (Yan et al., 2021), and DiffCSE (Chuang et al., 2022). On the other hand, the chosen supervised models are InferSent (Conneau et al., 2017), USE (Cer et al., 2018), SBERT (Reimers & Gurevych, 2019), CoSENT (Su, 2022), as well as su- pervised versions of SimCSE and ConSERT.  

Transfer STS Tasks For a fair comparison, we train AnglE with the NLI datasets MNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015) and then transfer it to evaluate seven STS benchmark datasets. The evaluation results are presented in Table 2. It is evident that AnglE-BERT and AnglE- LLaMA consistently outperform the baselines with a gain of    $0.80\%$   and    $0.{\bar{7}}2\%$   in average score, respectively, over the previous SOTA SimCSE-BERT and SimCSE-LLaMA. Note that supervised SBERT and CoSENT show lower results than other unsupervised contrastive learning models like SimCSE and DiffCSE. This difference might arise from the difference in data distributions between the training and test data in the transfer STS tasks. They struggle to effectively generalize to STS tasks when trained solely with NLI datasets. In contrast, contrastive learning models exhibit better generalization capabilities due to their alignment and uniformity features. Because AnglE optimizes both the supervised cosine objective and the in-batch negative objective. This can allow AnglE to generalize well in transfer STS tasks. Additionally, the angle optimization in AnglE mitigates the negative impact of the saturation zone in the cosine function to produce better performance than other baselines.  

Non-transfer STS Tasks To provide a comprehensive analysis, we also evaluate the performance of the baselines in the non-transfer setting. We train the baselines on the train set and evaluate them on the test or validation set. Two typical models, SimCSE and SBERT, representing contrastive and supervised learning, are compared with our model. The results of the non-transfer STS tasks are listed in Table 3, where we evaluate the baselines on four short-text datasets (MRPC, STS-B, QQP, and QNLI) and one long-text dataset (GitHub Issues Similarity Dataset). SimCSE notably performs poorly compared to SBERT and AnglE in the non-transfer setting. This is due to the limitation of the small-scale training set, as there are not enough samples for SimCSE to effectively learn repre- sentations. Furthermore, the datasets only provide pair-supervised data, namely    $(x,x^{\dot{+}})$   or    $(x,{\bar{x}}^{-})$  , which prevents SimCSE from utilizing its hard negative objective that relies on triple-supervised  

![](images/7c3760313ff08e60de6d8a27d35b4bc34218829c40f56d6c5dfb69b24a07cd14.jpg)  

Table 2: Text embedding performance on STS tasks. We report the Spearman’s correlation    $\rho\times$  100  of the “all” setting computed by SentEval. For supervised LLaMA-based models, we fine- tuned them using the LoRA (Hu et al., 2021) technique and used the prompt “ Summarize sentence

 { sentence }  in one word: ” sparked by (Jiang et al., 2023).  esults marked with  $\dagger$   are obtained from

 (Reimers & Gurevych, 2019), wh  results marked with  ‡  are retrieved from (Gao et al., 2021). Additionally, results marked with  ⋆ denote our own implementation using official code. For the remaining baselines, we refer to the corresponding original papers to obtain their results.  

data    $(x,x^{+},x^{-})$  . This limitation might affect its performance. On the other hand, AnglE consis- tently outperforms SBERT, achieving an absolute gain of    $5.52\%$  . This can support the idea that angle-optimized text embedding can mitigate the negative impact of the cosine function, resulting in better performance. Furthermore, we explore applying the long text model  $\mathrm{RANS}_{b a s e}$   (86M pa- rameters) (Li et al., 2023) as the backbone to test the performance on long text. The results show that AnglE-BERT outperforms AnglE-RAN across all short text datasets. This advantage might be attributed to the larger parameter size of BERT and its proficiency in handling short texts. How- ever, we observe a remarkable shift in long-text STS. AnglE-RAN outperforms AnglE-BERT in this scenario, suggesting that AnglE-RAN can handle long texts well despite having fewer parameters.  

![](images/f5eb5cf09842d5302dbec936ce16f24f4c3f2367b60472c61e75184f8b50b71c.jpg)  

Table 3: Results on the STS tasks. All baseline results are our implementation using the official code. Spearman’s correlation   $(\rho\times100)$   serves as the reported metric.  

In short, this evidence suggests AnglE’s superiority in transfer and non-transfer settings, its ability to produce high-quality text embeddings, and its robustness and adaptability to different backbones.  

To gain a deeper understanding of AnglE, we conducted an ablation study examining different ob- jectives and their effects. The results in table 4 indicate that AnglE shows improved performance with all three objectives. In particular, we observe that AnglE experiences a greater drop in perfor- mance without the angle objective than without the in-batch negative (ibn) objective. This suggests that angle optimization is more important than ibn in improving text embedding. Additionally, we find that using the angle objective alone yields performance close to that of using the cosine objec- tive alone, demonstrating the effectiveness of angle optimization. We also evaluated five different pooling strategies and found that the “cls” strategy performed the best. Finally, we compared the ibn with/without identical sentence pair (ISP) detection and found that ibn without ISP detection has about  $0.18\%$   performance drop than with. This indicates that ibn with ISP detection is effective.  

![](images/6a9d36a15c796e5f367c07500257e4b88df2b730166f3fbeb81555b7ef66ea50.jpg)  

![Table 5: Results of unsupervised and LLM super- vised models on the STS-B test set. For ChatGPT, LLaMA, and ChatGLM, we use the gpt-turbo-3.5, 7B LLaMA2, and 6B ChatGLM, respectively. ](images/af83358289b77f53736604c3e41e18a5b2cfad5635e384dd3f0163c900b26ab6.jpg)  

![](images/a5739bc27396088a70f069ea3d8a27ab5276227b42970ede6c3458406514ee9e.jpg)  
Figure 4: The procedures of the LLM-supervised learning. For the STS task, we use the prompt “ You are a highly smart same-meaning/opposite-meaning sentence-generating system. Your job is to generate    $\{s i z e\}$   synonymous/antonym sente  a giv ut sentence. Input sentence:    $\{t e x t\}$  . Output: ” to generate positive/negative pairs.  {  $\{\mathrm{size}\}$  }  and  {  $\{{\bf\mathrm{{ext}}}\}$  }  are placeholders for the generated size and the input text, respectively.  

# 4.5 D ISCUSSION AND  A NALYSIS  

Discussion of LLM-supervised Learning AnglE, a supervised learning model, must be trained on labeled data. However, the limited availability of domain-supervised data poses a challenge in real-world applications. To overcome this problem, we propose LLM-supervised learning. This approach applies LLMs as data annotators to label the pseudo-supervised data for AnglE training. Figure 4 outlines the procedures involved in LLM-supervised learning. In this study, we compare the LLM-supervised AnglE and unsupervised contrastive learning models. To simulate domain application, we extract all “sentence1” texts from the STS-B train set and employ LLM-supervised learning to train the AnglE model. Table 5 shows the results on the STS-B test set. It is evident from  

![](images/59f9e0d4a3ff2859df7c63aa6a3ce7c17ac785e849d21025efebc7abfd0a0de1.jpg)  
Figure 5: (a) Density plots of cosine similarities between sentence pairs in the STS-B test set. The pairs have been categorized into 6 groups, reflecting the ground truth ratings (where higher ratings indicate a higher degree of similarity), visually represented on the y-axis. The  $\mathbf{X}$  -axis represents the cosine similarity. (b) Density plots of golden scores between sentence pairs in the STS-B test set.  

the results that LLM-supervised AnglE performs better than unsupervised contrastive baselines, and the ensemble of LLMs shows the best results. This evidence suggests the effectiveness of LLM- supervised learning and indicates that it can alleviate the domain-supervised data scarcity problem.  

Discussion of Text Retrieval We also evaluate the performance of the text retrieval task by exper- imenting on the test split of the flick  $\cdot30\mathbf{k}$   dataset (Young et al., 2014). This dataset consists of five caption texts for each photo, and these texts are similar to each other. We use the first caption text vector to retrieve the top 5 similar sentences using faiss   5 . The strict accuracy   6   of AnglE, SimCSE (supervised), and SBERT are    $12.9\%$  ,    $10.4\%$  , and    $5.2\%$  , respectively. This evidence indicates the effectiveness of using AnglE for the retrieval task.  

Discussion of Transfer Tasks In addition, we evaluate the performance of text embedding in transfer tasks. In particular, our approach involves training text embedding on STS tasks and then transferring it to seven other kinds of tasks. Notably, AnglE outperforms baselines, showing a significant improvement of  $4.34\%$   and  $4.48\%$   over DiffCSE and SimCSE, respectively. These results suggest that AnglE can produce better embeddings that effectively improve performance in various tasks. A more detailed description of the experiment can be found in section A.2.  

Analysis of Text Embedding Distribution Figure 5a depicts the density plots of the cosine simi- larities between sentence pairs in the STS-B test set to provide an intuitive visualization of the text embedding quality. Figure 5b displays the golden scores for the same sentence pairs. Analyzing the overall density of cosine similarities, we find that AnglE’s distribution resembles the golden distri- bution more closely than SimCSE (supervised) and SBERT’s. Figure 5b illustrates a peak in the 0-1 range; however, only AnglE shows a distinct peak in this range in Figure 5a. Also, Figure 5b por- trays a higher peak around  4  than around  4 . 8  in the 4-5 range, only AnglE demonstrates this feature properly in Figure 5a. Notably, the 0-1 and 4-5 ranges in Figure 5b represent two saturation zones of the cosine function. This evidence suggests that AnglE can mitigate the negative effect of the sat- uration zone. In conclusion, we can confidently assert that AnglE produces better text embeddings with a cosine similarity density closely resembling the actual distribution than the baselines.  

# 5 C ONCLUSION AND  F UTURE  W ORK  

In this paper, we have presented a novel text embedding model called AnglE, which optimizes the angle difference in complex space to overcome the adverse impact of the saturation zone of the cosine function, thereby improving text embeddings. To comprehensively evaluate the STS tasks, we have introduced the GitHub Issues Similarity Dataset to evaluate model performance on the long-text STS task. Furthermore, we have proposed an LLM-supervised learning method to cope with the scarcity of domain-supervised data. Extensive experimental results have demonstrated that AnglE outperforms baselines, indicating that AnglE can handle both short and long-text STS tasks and work effectively in various scenarios. In future work, we plan to explore the application of AnglE in real-world scenarios and provide further insights into AnglE.  

# R EFERENCES  

Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: A pilot on semantic textual similarity. In  \*SEM 2012: The First Joint Conference on Lexical and Com- putational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012) , pp. 385–393. Association for Computational Linguistics, 2012.  

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. \*SEM 2013 shared task: Semantic textual similarity. In  Second Joint Conference on Lexical and Computational Se- mantics   $^{\prime\ast}\!S E\!M)$  , Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity , pp. 32–43, Atlanta, Georgia, USA, 2013. Association for Computational Lin- guistics.  

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei- wei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2014 task 10: Multilingual semantic textual similarity. In  Proceedings of the 8th International Workshop on Semantic Eval- uation (SemEval 2014) , pp. 81–91. Association for Computational Linguistics, 2014.  

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei- wei Guo, I˜ nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpret ability. In  Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) , pp. 252–263. Association for Computational Linguistics, 2015.  

Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger- man Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In  Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016) , pp. 497–511. Association for Computational Linguistics, 2016.  

Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts) , pp. 41–46. Association for Computational Linguistics, July 2023.  

Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno- tated corpus for learning natural language inference. In  Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pp. 632–642. Association for Computational Linguistics, 2015.  

Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylip¨ a¨ a Hellqvist, and Magnus Sahlgren. Semantic re-tuning with contrastive tension. In  International conference on learning representations , 2020.  

Daniel Cer, Mona Diab, Eneko Agirre, I˜ nigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In  Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) , pp. 1–14. Association for Computational Linguistics, August 2017.  

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Con- stant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. Univer- sal sentence encoder for English. In  Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 169–174. Association for Compu- tational Linguistics, 2018.  

Sachin Chanchani and Ruihong Huang. Composition-contrastive learning for sentence embeddings. In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics , pp. 15836–15848. Association for Computational Linguistics, 2023.  

Yung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-Wen Li, Scott Yih, Yoon Kim, and James Glass. DiffCSE: Difference-based contrastive learning for sentence embeddings. In  Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 4207–4218. Association for Computational Linguistics, 2022.  

Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence repre- sentations. In  Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) . European Language Resources Association (ELRA), 2018.  

Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ ıc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In  Proceed- ings of the 2017 Conference on Empirical Methods in Natural Language Processing , pp. 670–680. Association for Computational Linguistics, 2017.  

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In  Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 4171–4186, 2019.  

Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In  Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics , pp. 320–335, 2022.  

Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 6894–6910. Association for Computational Linguistics, 2021.  

John Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. DeCLUTR: Deep contrastive learning for un- supervised textual representations. In  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 879–895. Association for Computational Linguistics, 2021.  

Jean-Bastien Grill, Florian Strub, Florent Altch´ e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning.  Advances in neural information processing systems , 33:21271–21284, 2020.  

Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In  NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 1367– 1377. The Association for Computational Linguistics, 2016.  

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.  

Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, and Qi Zhang. PromptBERT: Improving BERT sentence embed- dings with prompts. In  Proceedings of the 2022 Conference on Empirical Methods in Natu- ral Language Processing , pp. 8826–8837. Association for Computational Linguistics, December 2022a. doi: 10.18653/v1/2022.emnlp-main.603. URL  https://aclanthology.org/2022. emnlp-main.603 .  

Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. Scaling sentence embeddings with large language models.  arXiv preprint arXiv:2307.16645 , 2023.  

Yuxin Jiang, Linhan Zhang, and Wei Wang. Improved universal sentence embeddings with prompt- based contrastive learning and energy-based learning. In  Findings of the Association for Compu- tational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pp. 3021–3035. Association for Computational Linguistics, 2022b.  

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. Skip-thought vectors. In  Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015 , pp. 3294–3302, 2015.  

Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence em- beddings from pre-trained language models. In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 9119–9130. Association for Computa- tional Linguistics, 2020.  

Xianming Li, Zongxi Li, Haoran Xie, and Qing Li. Merging statistical feature via adaptive gate for improved text classification. In  Proceedings of the AAAI conference on artificial intelligence , volume 35, pp. 13288–13296, 2021.  

Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, and Qing Li. Recurrent attention networks for long-text modeling. In  Findings of the Association for Computational Linguistics: ACL 2023 , pp. 3006–3019. Association for Computational Linguis- tics, 2023.  

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach.  CoRR , abs/1907.11692, 2019. URL  http://arxiv.org/abs/1907.11692 .  

Wenpeng Lu, Xu Zhang, Huimin Lu, and Fangfang Li. Deep hierarchical encoding model for sentence semantic matching.  Journal of Visual Communication and Image Representation , 71: 102794, 2020.  

Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. A SICK cure for the evaluation of compositional distributional semantic models. In  Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14) , pp. 216–223. European Language Resources Association (ELRA), 2014.  

Tom´ as Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositional it y. In  27th Annual Conference on Neural Information Processing Systems 2013. , pp. 3111–3119, 2013.  

OpenAI. Introducing chatgpt, 2022. URL  https://openai.com/blog/chatgpt .  

OpenAI. GPT-4 technical report.  CoRR , abs/2303.08774, 2023.  

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi. Unsupervised learning of sentence embed- dings using compositional n-gram features. In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, Volume 1 (Long Papers) , pp. 528–540. Association for Computational Linguistics, 2018.  

Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word rep- resentation. In  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 1532–1543. Association for Computational Linguistics, 2014.  

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert- networks. In  Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing , pp. 3980–3990. Association for Computational Linguistics, 2019.  

Jianlin Su. Cosent (1): A more effective sentence vector scheme than sentence bert, Jan 2022. URL https://kexue.fm/archives/8847 .  

Jianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. Whitening sentence representations for better semantics and faster retrieval.  arXiv preprint arXiv:2103.15316 , 2021.  

Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space.  arXiv preprint arXiv:1902.10197 , 2019.  

Varsha Suresh and Desmond Ong. Not all negatives are equal: Label-aware contrastive loss for fine-grained text classification. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 4381–4394. Association for Computational Linguistics, 2021.  

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ ee Lacroix, Baptiste Rozi\` ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.  arXiv preprint arXiv:2302.13971 , 2023a.  

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models.  arXiv preprint arXiv:2307.09288 , 2023b.  

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen- tence understanding through inference. In  Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long Papers) , pp. 1112–1122, New Orleans, Louisiana, 2018. Association for Computational Linguistics.  

Lingling Xu, Haoran Xie, Zongxi Li, Fu Lee Wang, Weiming Wang, and Qing Li. Contrastive learning models for sentence representations.  ACM Trans. Intell. Syst. Technol. , 14(4), jun 2023. ISSN 2157-6904. doi: 10.1145/3593590. URL  https://doi.org/10.1145/3593590 .  

Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. Consert: A contrastive framework for self-supervised sentence representation transfer. In  Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing , pp. 5065–5075. Association for Computa- tional Linguistics, 2021.  

Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.  Transactions of the Association for Computational Linguistics , 2:67–78, 2014.  

Zhenrui Yue, Bernhard Kratzwald, and Stefan Feuerriegel. Contrastive domain adaptation for ques- tion answering using limited text corpora. In  Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 9575–9593. Association for Computational Lin- guistics, November 2021.  

Han Zhang, Zongxi Li, Haoran Xie, Raymond YK Lau, Gary Cheng, Qing Li, and Dian Zhang. Leveraging statistical information in fine-grained financial sentiment analysis.  World Wide Web , 25(2):513–531, 2022.  

Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, and Lidong Bing. An unsupervised sentence embedding method by mutual information maximization. In  Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 1601–1610. Association for Computational Linguistics, 2020.  

Wenjie Zhuo, Yifan Sun, Xiaohan Wang, Linchao Zhu, and Yi Yang. WhitenedCSE: Whitening- based contrastive learning of sentence embeddings. In  Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics , pp. 12135–12148. Association for Computational Linguistics, 2023.  

# A A PPENDIX  

A.1 L IST OF  O PEN - SOURCE  P ROJECTS IN  G IT H UB  I SSUE  S IMILARITY  D ATASET  

We collected GitHub issues from the following  55  GitHub repositories.  

![](images/189090c071ba3ce2499f300ba01295da1ce414df5aae669a830a8e8e4e285cc2.jpg)  

# A.2 T RANSFER  T ASK  E XPERIMENT  

Table 6 presents the results of the transfer tasks. A comprehensive analysis reveals that AnglE out- performs baselines, achieving the best results in terms of average score, with 6 out of 7 best results. Notably, AnglE exhibits improvements of    $4.34\%$   and  $4.48\%$   over DiffCSE and SimCSE, respec- tively. This compelling evidence asserts that AnglE can produce better embeddings that effectively assist in various tasks.  

![](images/1815be27ef475ebff175a1bcaa3c39e837d722196b90d9d4e2fee604c04e3afc.jpg)  

Table 6: Transfer task results of different  entence embedding models (measu d as accuracy).  † : results from  imers & Gurevych (2019);  ‡ : results from Zhang et al. (2020);  ⋆ : results from Gao et al. (2021).  ♢ : results from Chuang et al. (2022).  